{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_virtual_device_configuration(physical_devices[0], [\n",
    "tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2000)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   questions  5000 non-null   object\n",
      " 1   a          5000 non-null   object\n",
      " 2   b          5000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 117.3+ KB\n",
      "None\n",
      "['TTD' 'TGU' 'ACM' 'TRS' 'WTH' 'FOD' 'ENT' 'TGU\\n' 'TTD\\n' '\\nENT']\n",
      "What are the companies which organize shark feeding events for scuba divers?\n"
     ]
    }
   ],
   "source": [
    "file_name = 'data/Travel-Dataset-5000--master/5000TravelQuestionsDataset.xlsx'\n",
    "test_id  = 1\n",
    "col_names = ['questions', 'a', 'b']\n",
    "dataset = pd.read_excel(file_name, header=None, names=col_names)\n",
    "dataset['questions'].dropna(inplace=True)\n",
    "print(dataset.info())\n",
    "print(dataset['a'].unique())\n",
    "print(dataset['questions'][test_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are the companies which organize shark feeding events for scuba divers '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    return text\n",
    "\n",
    "dataset['clean_questions'] = [clean_text(question) for question in dataset['questions']]\n",
    "dataset['clean_questions'][test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def lowercase(x):\n",
    "#     return x.lower()\n",
    "\n",
    "# dataset['questions'] = [clean_text(question) for question in dataset['questions']]\n",
    "# dataset['lc_questions'][test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "\n",
    "# def remove_punctuation(x):\n",
    "#     return \"\".join([char for char in x if char not in string.punctuation])\n",
    "\n",
    "# dataset['questions'] = [remove_punctuation(question) for question in dataset['questions']]\n",
    "# dataset['questions'][test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.corpus import stopwords\n",
    "# stop_words = stopwords.words('english')\n",
    "# from nltk import word_tokenize\n",
    "\n",
    "# def remove_stopwords(x):\n",
    "#     words = word_tokenize(x)\n",
    "#     return \" \".join([word for word in words if word not in stop_words])\n",
    "\n",
    "# dataset['questions'] = [remove_stopwords(question) for question in dataset['questions']]\n",
    "# dataset['questions'][test_id]\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.stem.porter import PorterStemmer\n",
    "# porter = PorterStemmer()\n",
    "# def stemming(x):\n",
    "#     filtered_words = word_tokenize(x['questions'])\n",
    "#     stemmed = [porter.stem(word) for word in filtered_words]\n",
    "#     return \" \".join(stemmed)\n",
    "\n",
    "# dataset['questions'] = [remove_stopwords(question) for question in dataset['questions']]\n",
    "# dataset['questions'][test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize and create BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are the best beach for shelling in capetown'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(x):\n",
    "    filtered_words = nltk.word_tokenize(x)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "dataset['lem_questions'] = [lemmatize(question) for question in dataset['clean_questions']]\n",
    "dataset['lem_questions'][test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WP VBP DT JJS NNS IN VBG IN NNP .'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "def pos_tagging(x):\n",
    "    words = nltk.word_tokenize(x)\n",
    "    lst = [ r[1] for r in pos_tag(words)] \n",
    "    return ' '.join(lst)\n",
    "\n",
    "dataset['pos_questions'] = [pos_tagging(question) for question in dataset['questions']]\n",
    "dataset['pos_questions'][test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dulan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'CapeTown'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if continuous_chunk:\n",
    "        named_entity = \" \".join(current_chunk)\n",
    "        if named_entity not in continuous_chunk:\n",
    "            continuous_chunk.append(named_entity)\n",
    "    \n",
    "    def remove_null(x):\n",
    "        if '' in x:\n",
    "            x.remove('')\n",
    "        return x\n",
    "\n",
    "    lst = remove_null(continuous_chunk)\n",
    "    return ' '.join(lst)\n",
    "\n",
    "txt = \"Barack Obama is a great person.\" \n",
    "txt2 = \"Who is Dulan?\"\n",
    "print (get_continuous_chunks(txt2))\n",
    "\n",
    "\n",
    "\n",
    "dataset['ne_questions'] = [get_continuous_chunks(question) for question in dataset['questions']]\n",
    "dataset['ne_questions'][test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1090)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_count_vect(documents):\n",
    "    vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "    X = vectorizer.fit_transform(documents).toarray()\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "print(get_count_vect(dataset['questions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Normal train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 5000)\n",
      "Fold - 1 - SVM Accuracy Score ->  - 83.20\n",
      "(4500, 5000)\n",
      "Fold - 2 - SVM Accuracy Score ->  - 84.80\n",
      "(4500, 5000)\n",
      "Fold - 3 - SVM Accuracy Score ->  - 81.40\n",
      "(4500, 5000)\n",
      "Fold - 4 - SVM Accuracy Score ->  - 81.80\n",
      "(4500, 5000)\n",
      "Fold - 5 - SVM Accuracy Score ->  - 83.40\n",
      "(4500, 5000)\n",
      "Fold - 6 - SVM Accuracy Score ->  - 84.20\n",
      "(4500, 5000)\n",
      "Fold - 7 - SVM Accuracy Score ->  - 83.40\n",
      "(4500, 5000)\n",
      "Fold - 8 - SVM Accuracy Score ->  - 84.40\n",
      "(4500, 5000)\n",
      "Fold - 9 - SVM Accuracy Score ->  - 82.80\n",
      "(4500, 5000)\n",
      "Fold - 10 - SVM Accuracy Score ->  - 82.80\n",
      "Mean 83.22 Std 1.03\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = dataset['lem_questions'],dataset['a']\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf.fit(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "fold = 0\n",
    "accuracies = []\n",
    "for train_index, test_index in cv.split(X):\n",
    "    fold += 1\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    y_train_e = le.transform(y_train)\n",
    "    y_test_e = le.transform(y_test)\n",
    "    \n",
    "    X_train_tfidf = tfidf.transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "    print(X_train_tfidf.shape)\n",
    "    \n",
    "    SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train_tfidf,y_train_e)\n",
    "    predictions_SVM = SVM.predict(X_test_tfidf)\n",
    "    acc = accuracy_score(predictions_SVM, y_test_e)*100\n",
    "    accuracies.append(acc)\n",
    "    print(\"Fold - {} - {} - {:.2f}\".format(fold, \"SVM Accuracy Score -> \",acc))\n",
    "    \n",
    "print(\"Mean {:.2f} Std {:.2f}\".format(np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 31)\n",
      "Fold - 1 - SVM Accuracy Score ->  - 38.80\n",
      "(4500, 31)\n",
      "Fold - 2 - SVM Accuracy Score ->  - 40.00\n",
      "(4500, 31)\n",
      "Fold - 3 - SVM Accuracy Score ->  - 39.80\n",
      "(4500, 31)\n",
      "Fold - 4 - SVM Accuracy Score ->  - 42.20\n",
      "(4500, 31)\n",
      "Fold - 5 - SVM Accuracy Score ->  - 41.80\n",
      "(4500, 31)\n",
      "Fold - 6 - SVM Accuracy Score ->  - 40.00\n",
      "(4500, 31)\n",
      "Fold - 7 - SVM Accuracy Score ->  - 39.40\n",
      "(4500, 31)\n",
      "Fold - 8 - SVM Accuracy Score ->  - 37.00\n",
      "(4500, 31)\n",
      "Fold - 9 - SVM Accuracy Score ->  - 40.20\n",
      "(4500, 31)\n",
      "Fold - 10 - SVM Accuracy Score ->  - 33.80\n",
      "Mean 39.30 Std 2.29\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X, y = dataset['pos_questions'],dataset['a']\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "tfidf.fit(X)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y)\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "fold = 0\n",
    "accuracies = []\n",
    "for train_index, test_index in cv.split(X):\n",
    "    fold += 1\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    y_train_e = le.transform(y_train)\n",
    "    y_test_e = le.transform(y_test)\n",
    "    \n",
    "    X_train_tfidf = tfidf.transform(X_train)\n",
    "    X_test_tfidf = tfidf.transform(X_test)\n",
    "    print(X_train_tfidf.shape)\n",
    "    \n",
    "    SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train_tfidf,y_train_e)\n",
    "    predictions_SVM = SVM.predict(X_test_tfidf)\n",
    "    acc = accuracy_score(predictions_SVM, y_test_e)*100\n",
    "    accuracies.append(acc)\n",
    "    print(\"Fold - {} - {} - {:.2f}\".format(fold, \"SVM Accuracy Score -> \",acc))\n",
    "    \n",
    "print(\"Mean {:.2f} Std {:.2f}\".format(np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy Evauluation on Different Matrices\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def PRC_matrics(y_test, prediction):\n",
    "    # calculate prediction\n",
    "    precision = precision_score(y_test, prediction, labels=[1,2], average='micro')*100\n",
    "    print('Precision: %.3f' % precision)\n",
    "\n",
    "    # calculate recall\n",
    "    recall = recall_score(y_test, prediction, labels=[1,2], average='micro')*100\n",
    "    print('Recall: %.3f' % recall)\n",
    "    \n",
    "    # calculate score\n",
    "#     score = f1_score(y_test, prediction, average='micro')\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    print('F-Measure: %.3f' % f1_score)\n",
    "    \n",
    "    \n",
    "    acc = accuracy_score(y_test, prediction)*100\n",
    "    tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "    acc2 = (tn + tp)*100/(tn + fp + fn + tp)\n",
    "    print('Accuracy score: %.3f' % acc2)\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_test, prediction)\n",
    "    print(\"Confustion matrix: \\n{}\".format(cm))\n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - A traditional ML classifier s.a. SVM or Logistic Regression with at least 5  of the features mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 26)\n",
      "(5000, 322)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 5348)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.sparse import coo_matrix, csr_matrix, hstack\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(dataset['a'])\n",
    "\n",
    "X_lem = dataset['lem_questions']\n",
    "tfidf_lem = TfidfVectorizer(max_features=5000)\n",
    "tfidf_lem.fit(X_lem)\n",
    "\n",
    "X_pos = dataset['pos_questions']\n",
    "\n",
    "X_ne = dataset['ne_questions']\n",
    "\n",
    "XX = csr_matrix(hstack([tfidf_lem.transform(X_lem) ,get_count_vect(X_pos), get_count_vect(X_ne)]))\n",
    "\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['lem_questions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 5348)\n",
      "Fold - 1 - SVM Accuracy Score ->  - 79.20\n",
      "(4500, 5348)\n",
      "Fold - 2 - SVM Accuracy Score ->  - 82.20\n",
      "(4500, 5348)\n",
      "Fold - 3 - SVM Accuracy Score ->  - 79.00\n",
      "(4500, 5348)\n",
      "Fold - 4 - SVM Accuracy Score ->  - 79.80\n",
      "(4500, 5348)\n",
      "Fold - 5 - SVM Accuracy Score ->  - 81.00\n",
      "(4500, 5348)\n",
      "Fold - 6 - SVM Accuracy Score ->  - 80.80\n",
      "(4500, 5348)\n",
      "Fold - 7 - SVM Accuracy Score ->  - 81.00\n",
      "(4500, 5348)\n",
      "Fold - 8 - SVM Accuracy Score ->  - 78.80\n",
      "(4500, 5348)\n",
      "Fold - 9 - SVM Accuracy Score ->  - 80.60\n",
      "(4500, 5348)\n",
      "Fold - 10 - SVM Accuracy Score ->  - 80.00\n",
      "Mean 80.24 Std 1.02\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "fold = 0\n",
    "accuracies = []\n",
    "for train_index, test_index in cv.split(XX):\n",
    "    fold += 1\n",
    "    X_train, X_test = XX[train_index], XX[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    print(X_train.shape)\n",
    "\n",
    "    \n",
    "    SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train,y_train)\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    acc = accuracy_score(predictions_SVM, y_test)*100\n",
    "    accuracies.append(acc)\n",
    "    print(\"Fold - {} - {} - {:.2f}\".format(fold, \"SVM Accuracy Score -> \",acc))\n",
    "    \n",
    "print(\"Mean {:.2f} Std {:.2f}\".format(np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.856\n",
      "Recall: 0.778\n",
      "F-Measure: 0.800\n",
      "Accuracy score: 80.000\n",
      "Confustion matrix: \n",
      "[[63  0  1  4  0  8  0]\n",
      " [ 0 14  1  2  1  5  0]\n",
      " [ 3  1 47  4  0  5  0]\n",
      " [ 4  0  1 85  7 15  0]\n",
      " [ 2  1  0  8 83  4  0]\n",
      " [ 2  0  0 15  3 93  1]\n",
      " [ 0  0  0  2  0  0 15]]\n"
     ]
    }
   ],
   "source": [
    "PRC_matrics(y_test, predictions_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Doc2Vec - Using Sentence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset['questions'])]\n",
    "model = Doc2Vec(documents, vector_size=1000, window=2, min_count=1, workers=4)\n",
    "\n",
    "def doc2vec(x):\n",
    "    return np.array(model.infer_vector(x.split(' ')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000,)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec('hello world').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# _id = 3\n",
    "# vector = model.infer_vector(dataset['questions'][_id].split(' '))\n",
    "# print(vector)\n",
    "# print(dataset['questions'][_id])\n",
    "\n",
    "dataset['doc2vec_questions'] = [doc2vec(question) for question in dataset['questions']]\n",
    "dataset['doc2vec_questions'][test_id]\n",
    "dataset['doc2vec_questions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 1000)\n",
      "(4500,)\n",
      "Fold - 1 - SVM Accuracy Score ->  - 21.20\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  81   0   0   0]\n",
      " [  0   0   0  25   0   0   0]\n",
      " [  0   0   0  56   0   0   0]\n",
      " [  0   0   0 106   0   0   0]\n",
      " [  0   0   0  96   0   0   0]\n",
      " [  0   0   0 118   0   0   0]\n",
      " [  0   0   0  18   0   0   0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 2 - SVM Accuracy Score ->  - 27.80\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 0  0  0 26  0 56  0  0]\n",
      " [ 0  0  0  3  0 22  0  0]\n",
      " [ 0  0  0 13  0 26  0  0]\n",
      " [ 0  0  0 54  0 70  0  0]\n",
      " [ 0  0  0 26  0 68  0  0]\n",
      " [ 0  0  0 25  0 85  0  0]\n",
      " [ 0  0  0  1  0  0  0  0]\n",
      " [ 0  0  0  4  0 21  0  0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 3 - SVM Accuracy Score ->  - 25.60\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 0  0  0 11  0  0 61  0]\n",
      " [ 0  0  0  3  0  0 13  0]\n",
      " [ 0  0  0  9  0  0 43  0]\n",
      " [ 0  0  0 35  0  0 90  0]\n",
      " [ 0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0 20  0  0 78  0]\n",
      " [ 0  0  0 27  0  0 93  0]\n",
      " [ 0  0  0  2  0  0 14  0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 4 - SVM Accuracy Score ->  - 23.80\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 0  0  0 10  0 60  0]\n",
      " [ 0  0  0  1  0 23  0]\n",
      " [ 0  0  0  8  0 49  0]\n",
      " [ 0  0  0 40  0 84  0]\n",
      " [ 0  0  0 21  0 81  0]\n",
      " [ 0  0  0 27  0 79  0]\n",
      " [ 0  0  0  4  0 13  0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 5 - SVM Accuracy Score ->  - 23.80\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 0  0  0 12  0 45  0]\n",
      " [ 0  0  0  4  0 26  0]\n",
      " [ 0  0  0 17  0 32  0]\n",
      " [ 0  0  0 30  0 95  0]\n",
      " [ 0  0  0 23  0 87  0]\n",
      " [ 0  0  0 25  0 89  0]\n",
      " [ 0  0  0  1  0 14  0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 6 - SVM Accuracy Score ->  - 24.20\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  10   0  49   0]\n",
      " [  0   0   0   4   0  10   0]\n",
      " [  0   0   0   9   0  41   0]\n",
      " [  0   0   0  37   0 111   0]\n",
      " [  0   0   0  24   0  86   0]\n",
      " [  0   0   0  17   0  84   0]\n",
      " [  0   0   0   1   0  17   0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 7 - SVM Accuracy Score ->  - 22.00\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0  89   0   0   0]\n",
      " [  0   0   0   0  24   0   0   0]\n",
      " [  0   0   0   0  43   0   0   0]\n",
      " [  0   0   0   0 110   0   0   0]\n",
      " [  0   0   0   0  98   0   0   0]\n",
      " [  0   0   0   0 120   0   0   0]\n",
      " [  0   0   0   0  15   0   0   0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 8 - SVM Accuracy Score ->  - 23.80\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 0  0  0 16  0  0 47  0]\n",
      " [ 0  0  0  3  0  0 13  0]\n",
      " [ 0  0  0 13  0  0 48  0]\n",
      " [ 0  0  0 28  0  0 89  0]\n",
      " [ 0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0 26  0  0 84  0]\n",
      " [ 0  0  0 24  0  0 91  0]\n",
      " [ 0  0  0  1  0  0 16  0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 9 - SVM Accuracy Score ->  - 27.40\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0 18  0  0 53  0]\n",
      " [ 0  0  0  0  3  0  0 14  0]\n",
      " [ 0  0  0  0 10  0  0 44  0]\n",
      " [ 0  0  0  0 45  0  0 81  0]\n",
      " [ 0  0  0  0  0  0  0  1  0]\n",
      " [ 0  0  0  0 26  0  0 69  0]\n",
      " [ 0  0  0  0 29  0  0 92  0]\n",
      " [ 0  0  0  0  3  0  0 11  0]]\n",
      "(4500, 1000)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 10 - SVM Accuracy Score ->  - 24.20\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 0  0  0 26  0 50  0]\n",
      " [ 0  0  0  6  0 17  0]\n",
      " [ 0  0  0 21  0 39  0]\n",
      " [ 0  0  0 31  0 81  0]\n",
      " [ 0  0  0 22  0 76  0]\n",
      " [ 0  0  0 24  0 90  0]\n",
      " [ 0  0  0  2  0 15  0]]\n",
      "Mean 24.38 Std 1.98\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_doc2vec = np.array([doc2vec(question) for question in dataset['questions']])\n",
    "le = LabelEncoder()\n",
    "y_doc2vec = le.fit_transform(dataset['a'])\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "fold = 0\n",
    "accuracies = []\n",
    "for train_index, test_index in cv.split(X_doc2vec):\n",
    "    fold += 1\n",
    "    X_train, X_test = X_doc2vec[train_index], X_doc2vec[test_index]\n",
    "    y_train, y_test = y_doc2vec[train_index], y_doc2vec[test_index]\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train,y_train)\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    acc = accuracy_score(predictions_SVM, y_test)*100\n",
    "    accuracies.append(acc)\n",
    "    print(\"Fold - {} - {} - {:.2f}\".format(fold, \"SVM Accuracy Score -> \",acc))\n",
    "    PRC_matrics(y_test, predictions_SVM)\n",
    "    \n",
    "print(\"Mean {:.2f} Std {:.2f}\".format(np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 7, 1, 6, 1, 4, 9, 7])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[1:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"fasttext_skipgram_travel_questions.bin\" - model loaded\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import os\n",
    "model_name='fasttext_skipgram_travel_questions.bin'\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    dataset['questions'].to_csv('questions.txt', sep='.', header=False, index=False)\n",
    "    model_fasttext = fasttext.train_unsupervised('questions.txt', model='skipgram')\n",
    "    model_fasttext.save_model(model_name)\n",
    "    print(\"Model saved as {}\".format(model_name))\n",
    "else:\n",
    "    print(\"\\\"{}\\\" - model loaded\".format(model_name))\n",
    "    model_fasttext = fasttext.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fasttext.words\n",
    "\n",
    "len(model_fasttext.get_word_vector(\"the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_text_mean_transform(X):\n",
    "    words = X.split(' ')    \n",
    "    return np.mean([model_fasttext.get_word_vector(w) for w in words if w in model_fasttext.words]\n",
    "                    or [np.zeros(100)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_text_first_x_words(X, length):\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    X_lstm = pad_sequences(X, maxlen=length)\n",
    "    \n",
    "    words = X.split(' ')    \n",
    "    return np.mean([model_fasttext.get_word_vector(w) for w in words if w in model_fasttext.words]\n",
    "                    or [np.zeros(100)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'the'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-76a1d0406b3a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'the jaya'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpad_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/keras/preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m    156\u001b[0m   return sequence.pad_sequences(\n\u001b[1;32m    157\u001b[0m       \u001b[0msequences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m keras_export(\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/keras_preprocessing/sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[0;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# check `trunc` has expected shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0mtrunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'the'"
     ]
    }
   ],
   "source": [
    "inp = 'the jaya'\n",
    "length = 5\n",
    "pad_sequences(inp.split(' '), maxlen=length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.43696733e-03,  2.11618513e-01, -2.26052940e-01, -1.66629389e-01,\n",
       "       -3.07577904e-02,  2.29724661e-01, -2.17954770e-01,  2.47001220e-02,\n",
       "       -1.19443396e-02,  6.16670437e-02,  1.59915537e-02,  4.66026105e-02,\n",
       "        1.32551521e-01, -3.69257145e-02,  1.09452471e-01, -4.01505679e-02,\n",
       "       -1.66812330e-01, -1.46364361e-01, -2.03689471e-01, -3.73825043e-01,\n",
       "       -1.85486615e-01,  3.38705212e-01,  1.30375087e-01, -1.01271719e-01,\n",
       "       -7.30609894e-02, -3.02379895e-02,  2.56874561e-01, -2.11235180e-01,\n",
       "        1.32814214e-01,  4.00184512e-01,  1.05642729e-01, -1.96828574e-01,\n",
       "        1.03269204e-01, -2.53339529e-01, -1.14064552e-01,  1.85176104e-01,\n",
       "       -1.17873266e-01, -6.13354146e-02, -1.77823022e-01,  3.01203458e-03,\n",
       "        1.38162136e-01,  3.44742656e-01, -1.57955641e-04, -4.63928692e-02,\n",
       "        2.69639529e-02, -2.69629329e-01,  5.92334010e-02, -1.76105440e-01,\n",
       "       -4.76358738e-03,  1.41994506e-01,  3.82607765e-02,  1.57244921e-01,\n",
       "       -8.58828332e-03,  1.11122750e-01,  2.08134666e-01,  7.22984299e-02,\n",
       "       -4.11947928e-02, -3.18960361e-02, -4.87852059e-02,  3.81737202e-02,\n",
       "        8.67851600e-02,  1.41748950e-01,  1.85773268e-01,  2.30206773e-01,\n",
       "       -9.11069885e-02, -1.28113646e-02, -3.83044146e-02,  1.73720550e-02,\n",
       "       -5.57320789e-02, -1.79843605e-01,  6.14141710e-02,  2.33513534e-01,\n",
       "        8.53596702e-02,  1.00520507e-01, -3.24510187e-02, -1.18235424e-01,\n",
       "       -5.18382639e-02,  3.56370181e-01, -7.82849044e-02,  7.86052831e-03,\n",
       "       -1.17493667e-01,  2.64396518e-01,  2.26080716e-01,  6.27642646e-02,\n",
       "        2.56612241e-01, -3.96920852e-02, -2.02297226e-01,  8.73202384e-02,\n",
       "       -1.57638252e-01, -3.28039937e-02,  2.18991786e-01,  2.60402411e-01,\n",
       "        2.55481094e-01, -1.19351670e-01, -1.71753302e-01, -9.02542844e-02,\n",
       "       -1.89402893e-01,  2.70825952e-01, -1.99194610e-01,  3.55222374e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(fast_text_mean_transform('dulan jaya'))\n",
    "fast_text_mean_transform('the jaya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['doc2fast_questions'] = [fast_text_mean_transform(question) for question in dataset['questions']]\n",
    "dataset['doc2fast_questions'][test_id]\n",
    "dataset['doc2fast_questions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 100)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 100)\n",
      "(4500, 100)\n",
      "(4500,)\n",
      "Fold - 1 - SVM Accuracy Score ->  - 21.20\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  81   0   0   0]\n",
      " [  0   0   0  25   0   0   0]\n",
      " [  0   0   0  56   0   0   0]\n",
      " [  0   0   0 105   1   0   0]\n",
      " [  0   0   0  95   1   0   0]\n",
      " [  0   0   0 118   0   0   0]\n",
      " [  0   0   0  18   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 2 - SVM Accuracy Score ->  - 24.80\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  82   0   0   0   0]\n",
      " [  0   0   0  25   0   0   0   0]\n",
      " [  0   0   0  39   0   0   0   0]\n",
      " [  0   0   0 124   0   0   0   0]\n",
      " [  0   0   0  94   0   0   0   0]\n",
      " [  0   0   0 110   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0]\n",
      " [  0   0   0  25   0   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 3 - SVM Accuracy Score ->  - 24.80\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  72   0   0   0   0]\n",
      " [  0   0   0  16   0   0   0   0]\n",
      " [  0   0   0  52   0   0   0   0]\n",
      " [  0   0   0 124   0   1   0   0]\n",
      " [  0   0   0   1   0   0   0   0]\n",
      " [  0   0   0  98   0   0   0   0]\n",
      " [  0   0   0 120   0   0   0   0]\n",
      " [  0   0   0  16   0   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 4 - SVM Accuracy Score ->  - 24.80\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  70   0   0   0]\n",
      " [  0   0   0  24   0   0   0]\n",
      " [  0   0   0  57   0   0   0]\n",
      " [  0   0   0 124   0   0   0]\n",
      " [  0   0   0 102   0   0   0]\n",
      " [  0   0   0 106   0   0   0]\n",
      " [  0   0   0  17   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 5 - SVM Accuracy Score ->  - 25.00\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  57   0   0   0]\n",
      " [  0   0   0  30   0   0   0]\n",
      " [  0   0   0  49   0   0   0]\n",
      " [  0   0   0 125   0   0   0]\n",
      " [  0   0   0 110   0   0   0]\n",
      " [  0   0   0 114   0   0   0]\n",
      " [  0   0   0  15   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 6 - SVM Accuracy Score ->  - 29.60\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  59   0   0   0]\n",
      " [  0   0   0  14   0   0   0]\n",
      " [  0   0   0  50   0   0   0]\n",
      " [  0   0   0 148   0   0   0]\n",
      " [  0   0   0 110   0   0   0]\n",
      " [  0   0   0 101   0   0   0]\n",
      " [  0   0   0  18   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 7 - SVM Accuracy Score ->  - 22.00\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0   0   1   0   0   0]\n",
      " [  0   0   0   0  89   0   0   0]\n",
      " [  0   0   0   0  24   0   0   0]\n",
      " [  0   0   0   0  43   0   0   0]\n",
      " [  0   0   0   0 110   0   0   0]\n",
      " [  0   0   0   0  98   0   0   0]\n",
      " [  0   0   0   0 120   0   0   0]\n",
      " [  0   0   0   0  15   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 8 - SVM Accuracy Score ->  - 23.40\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  62   0   1   0   0]\n",
      " [  0   0   0  16   0   0   0   0]\n",
      " [  0   0   0  61   0   0   0   0]\n",
      " [  0   0   0 117   0   0   0   0]\n",
      " [  0   0   0   1   0   0   0   0]\n",
      " [  0   0   0 110   0   0   0   0]\n",
      " [  0   0   0 115   0   0   0   0]\n",
      " [  0   0   0  17   0   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 9 - SVM Accuracy Score ->  - 25.20\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0  71   0   0   0   0]\n",
      " [  0   0   0   0  17   0   0   0   0]\n",
      " [  0   0   0   0  54   0   0   0   0]\n",
      " [  0   0   0   0 126   0   0   0   0]\n",
      " [  0   0   0   0   1   0   0   0   0]\n",
      " [  0   0   0   0  95   0   0   0   0]\n",
      " [  0   0   0   0 121   0   0   0   0]\n",
      " [  0   0   0   0  14   0   0   0   0]]\n",
      "(4500, 100)\n",
      "(4500,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 10 - SVM Accuracy Score ->  - 22.40\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0  74   2   0   0]\n",
      " [  0   0   0  23   0   0   0]\n",
      " [  0   0   0  60   0   0   0]\n",
      " [  0   0   0 112   0   0   0]\n",
      " [  0   0   0  98   0   0   0]\n",
      " [  0   0   0 114   0   0   0]\n",
      " [  0   0   0  17   0   0   0]]\n",
      "Mean 24.32 Std 2.22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:21: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "X_doc2fast = np.array([fast_text_mean_transform(question) for question in dataset['questions']])\n",
    "le = LabelEncoder()\n",
    "y_doc2fast = le.fit_transform(dataset['a'])\n",
    "\n",
    "print(X_doc2fast.shape)\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "fold = 0\n",
    "accuracies = []\n",
    "for train_index, test_index in cv.split(X_doc2fast):\n",
    "    fold += 1\n",
    "    X_train, X_test = X_doc2fast[train_index], X_doc2fast[test_index]\n",
    "    y_train, y_test = y_doc2fast[train_index], y_doc2fast[test_index]\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(y_train.shape)\n",
    "    \n",
    "    SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "    SVM.fit(X_train,y_train)\n",
    "    predictions_SVM = SVM.predict(X_test)\n",
    "    acc = accuracy_score(predictions_SVM, y_test)*100\n",
    "    accuracies.append(acc)\n",
    "    print(\"Fold - {} - {} - {:.2f}\".format(fold, \"SVM Accuracy Score -> \",acc))\n",
    "    PRC_matrics(y_test, predictions_SVM)\n",
    "    \n",
    "print(\"Mean {:.2f} Std {:.2f}\".format(np.mean(accuracies), np.std(accuracies)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 4, 4, 4, 4, 4, 4, 4])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_SVM[1:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PRC_matrics(y_test, predictions_SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a word Embedding Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A NN classifier s.a. an LSTM for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 5000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 160"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5582 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, split=' ')\n",
    "tokenizer.fit_on_texts(dataset['questions'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 2, 321, 105, 31, 1837, 17, 68, 9, 20, 71, 6, 194, 48, 32, 22, 376, 111]\n",
      "Shape of data tensor: (5000, 25)\n",
      "Shape of label tensor: (5000, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[4, 7, 2, 321, 105, 31, 1837, 17, 68, 9, 20, 71, 6, 194, 48, 32, 22, 376, 111]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = tokenizer.texts_to_sequences(dataset['questions'].values)\n",
    "print(X[0])\n",
    "X_lstm = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_lstm.shape)\n",
    "\n",
    "y_nn = pd.get_dummies(dataset['a']).values\n",
    "print('Shape of label tensor:', y_nn.shape)\n",
    "\n",
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_4 (Spatial (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 10)                1970      \n",
      "=================================================================\n",
      "Total params: 1,081,858\n",
      "Trainable params: 1,081,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f6acc19d810>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "def get_lstm_model(verbose=0):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_lstm.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
    "    # model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    epochs = 5\n",
    "    batch_size = 64\n",
    "    \n",
    "    if verbose == 1:\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "get_lstm_model(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 25) (4500, 10)\n",
      "(500, 25) (500, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_lstm,y_nn, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_15 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 3s 52ms/step - loss: 1.7949 - accuracy: 0.2899 - val_loss: 1.7669 - val_accuracy: 0.1378\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 3s 49ms/step - loss: 1.3039 - accuracy: 0.5119 - val_loss: 1.2886 - val_accuracy: 0.5756\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 3s 49ms/step - loss: 0.7133 - accuracy: 0.7709 - val_loss: 0.8973 - val_accuracy: 0.7200\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 3s 49ms/step - loss: 0.3285 - accuracy: 0.9044 - val_loss: 0.7604 - val_accuracy: 0.7711\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 3s 50ms/step - loss: 0.1609 - accuracy: 0.9548 - val_loss: 0.8369 - val_accuracy: 0.7556\n",
      "Precision: 0.821\n",
      "Recall: 0.798\n",
      "F-Measure: 0.832\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[70  0  3  6  0  3  0]\n",
      " [ 2 17  2  0  1  5  0]\n",
      " [ 1  2 45  1  0  0  0]\n",
      " [ 3  1  5 83  1  7  2]\n",
      " [ 3  2  1  9 92  7  0]\n",
      " [ 2  3  0  7  2 96  3]\n",
      " [ 0  0  0  0  0  0 13]]\n",
      "WARNING:tensorflow:Layer lstm_16 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 3s 53ms/step - loss: 1.7851 - accuracy: 0.3027 - val_loss: 1.7593 - val_accuracy: 0.1778\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 3s 49ms/step - loss: 1.2165 - accuracy: 0.5543 - val_loss: 1.3605 - val_accuracy: 0.4800\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 3s 51ms/step - loss: 0.5789 - accuracy: 0.8163 - val_loss: 1.1661 - val_accuracy: 0.5600\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 3s 51ms/step - loss: 0.2841 - accuracy: 0.9185 - val_loss: 1.0496 - val_accuracy: 0.6711\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 3s 51ms/step - loss: 0.1438 - accuracy: 0.9600 - val_loss: 0.7874 - val_accuracy: 0.7667\n",
      "Precision: 0.804\n",
      "Recall: 0.851\n",
      "F-Measure: 0.828\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 70   1   4   1   0   3   0   0]\n",
      " [  0  16   0   1   0   2   0   3]\n",
      " [  1   0  33   1   0   0   0   0]\n",
      " [  6   1   1 110  15   8   0   0]\n",
      " [  0   1   0   3  74   2   0   1]\n",
      " [  5   6   1   8   5  95   1   5]\n",
      " [  0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0   0   0  16]]\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 4s 57ms/step - loss: 1.8270 - accuracy: 0.2877 - val_loss: 2.0083 - val_accuracy: 0.1467\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 4s 57ms/step - loss: 1.3731 - accuracy: 0.4923 - val_loss: 1.4063 - val_accuracy: 0.4689\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 3s 53ms/step - loss: 0.7321 - accuracy: 0.7637 - val_loss: 1.0146 - val_accuracy: 0.6489\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 3s 52ms/step - loss: 0.3365 - accuracy: 0.8978 - val_loss: 0.8486 - val_accuracy: 0.7533\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 3s 52ms/step - loss: 0.1700 - accuracy: 0.9538 - val_loss: 0.7888 - val_accuracy: 0.7689\n",
      "Precision: 0.761\n",
      "Recall: 0.827\n",
      "F-Measure: 0.792\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[57  1  2  6  0  0  2  0]\n",
      " [ 0 10  1  0  0  0  2  0]\n",
      " [ 3  1 43  0  0  0  4  0]\n",
      " [ 5  1  1 92  1  6 16  0]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 1  0  1 11  0 92  6  1]\n",
      " [ 6  3  4 15  0  0 90  3]\n",
      " [ 0  0  0  1  0  0  0 12]]\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 3s 54ms/step - loss: 1.8051 - accuracy: 0.2790 - val_loss: 1.6753 - val_accuracy: 0.5933\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 3s 52ms/step - loss: 1.2429 - accuracy: 0.5454 - val_loss: 1.4567 - val_accuracy: 0.4800\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 3s 53ms/step - loss: 0.6675 - accuracy: 0.7800 - val_loss: 1.0034 - val_accuracy: 0.6822\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.3440 - accuracy: 0.9012 - val_loss: 0.9977 - val_accuracy: 0.6867\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.1822 - accuracy: 0.9481 - val_loss: 0.7049 - val_accuracy: 0.7689\n",
      "Precision: 0.713\n",
      "Recall: 0.817\n",
      "F-Measure: 0.788\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[59  2  2  4  1  5  0]\n",
      " [ 0  8  1  0  0  0  0]\n",
      " [ 0  1 47  1  0  2  0]\n",
      " [ 6  1  4 96  9 10  3]\n",
      " [ 1  0  0 11 88  6  0]\n",
      " [ 4 11  3 11  4 83  1]\n",
      " [ 0  1  0  1  0  0 13]]\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 1.8020 - accuracy: 0.2825 - val_loss: 1.8128 - val_accuracy: 0.3978\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 1.1666 - accuracy: 0.5968 - val_loss: 1.4532 - val_accuracy: 0.4089\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.5513 - accuracy: 0.8220 - val_loss: 0.8363 - val_accuracy: 0.7400\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 4s 66ms/step - loss: 0.2932 - accuracy: 0.9126 - val_loss: 0.9052 - val_accuracy: 0.7089\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 4s 65ms/step - loss: 0.1783 - accuracy: 0.9563 - val_loss: 0.8485 - val_accuracy: 0.7244\n",
      "Precision: 0.839\n",
      "Recall: 0.608\n",
      "F-Measure: 0.770\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[48  1  3  7  1 11  0]\n",
      " [ 2 25  0  0  2 19  1]\n",
      " [ 1  2 41  3  0  3  1]\n",
      " [ 3  0  2 96  5 10  3]\n",
      " [ 0  1  1  7 99  4  0]\n",
      " [ 3  1  2 12  3 66  0]\n",
      " [ 0  0  0  0  0  1 10]]\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 1.7919 - accuracy: 0.2822 - val_loss: 1.7771 - val_accuracy: 0.5311\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 1.2342 - accuracy: 0.5615 - val_loss: 1.2273 - val_accuracy: 0.5978\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.5872 - accuracy: 0.8128 - val_loss: 0.7438 - val_accuracy: 0.7711\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.2853 - accuracy: 0.9173 - val_loss: 1.0821 - val_accuracy: 0.6578\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.1718 - accuracy: 0.9538 - val_loss: 0.9109 - val_accuracy: 0.7178\n",
      "Precision: 0.836\n",
      "Recall: 0.772\n",
      "F-Measure: 0.816\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 50   1   0   4   0   4   0]\n",
      " [  0  11   2   1   0   6   0]\n",
      " [  3   0  45   2   2   0   0]\n",
      " [  3   0   1 111   5  12   1]\n",
      " [  1   0   0  18 102   7   0]\n",
      " [  2   2   2  12   1  72   0]\n",
      " [  0   0   0   0   0   0  17]]\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 4s 64ms/step - loss: 1.8210 - accuracy: 0.2711 - val_loss: 1.9235 - val_accuracy: 0.1511\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 1.2070 - accuracy: 0.5558 - val_loss: 1.2802 - val_accuracy: 0.5622\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 4s 64ms/step - loss: 0.5867 - accuracy: 0.8128 - val_loss: 0.7992 - val_accuracy: 0.7644\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.3048 - accuracy: 0.9094 - val_loss: 0.8589 - val_accuracy: 0.7422\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.1811 - accuracy: 0.9479 - val_loss: 0.9216 - val_accuracy: 0.7311\n",
      "Precision: 0.743\n",
      "Recall: 0.816\n",
      "F-Measure: 0.784\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[ 0  0  0  0  0  0  0  0]\n",
      " [ 0 72  0  0  6  1  5  0]\n",
      " [ 1  0 12  0  0  0  5  1]\n",
      " [ 0  2  5 42  4  0  7  0]\n",
      " [ 0 10  3  1 91  6 21  5]\n",
      " [ 0  3  1  0  3 89  4  0]\n",
      " [ 0  2  3  0  6  2 78  1]\n",
      " [ 0  0  0  0  0  0  0  8]]\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "64/64 [==============================] - 4s 63ms/step - loss: 1.7887 - accuracy: 0.2911 - val_loss: 1.9961 - val_accuracy: 0.1511\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 1.2641 - accuracy: 0.5348 - val_loss: 1.4981 - val_accuracy: 0.4778\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.6298 - accuracy: 0.7998 - val_loss: 0.9460 - val_accuracy: 0.6800\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.3513 - accuracy: 0.8956 - val_loss: 1.1021 - val_accuracy: 0.6444\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.1820 - accuracy: 0.9469 - val_loss: 0.8381 - val_accuracy: 0.7400\n",
      "Precision: 0.835\n",
      "Recall: 0.750\n",
      "F-Measure: 0.802\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[56  0  0  5  0  1  3  0]\n",
      " [ 2 10  3  2  0  0  4  2]\n",
      " [ 0  2 46  0  0  0  0  0]\n",
      " [ 3  0  6 90  0  9 14  1]\n",
      " [ 0  0  0  0  0  0  0  0]\n",
      " [ 1  0  0  7  0 99  6  1]\n",
      " [ 1  3  6 11  1  1 88  1]\n",
      " [ 0  1  0  2  0  0  0 12]]\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "64/64 [==============================] - 4s 64ms/step - loss: 1.8323 - accuracy: 0.2677 - val_loss: 1.8600 - val_accuracy: 0.1378\n",
      "Epoch 2/5\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 1.3131 - accuracy: 0.5193 - val_loss: 1.4218 - val_accuracy: 0.5289\n",
      "Epoch 3/5\n",
      "64/64 [==============================] - 4s 63ms/step - loss: 0.6185 - accuracy: 0.7983 - val_loss: 1.0205 - val_accuracy: 0.6756\n",
      "Epoch 4/5\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.2763 - accuracy: 0.9195 - val_loss: 0.6735 - val_accuracy: 0.7956\n",
      "Epoch 5/5\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.1453 - accuracy: 0.9593 - val_loss: 0.6972 - val_accuracy: 0.7889\n",
      "Precision: 0.830\n",
      "Recall: 0.785\n",
      "F-Measure: 0.838\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[  0   0   0   0   0   0   0   0   0]\n",
      " [  0  60   0   2   5   0   0   3   0]\n",
      " [  0   1  13   3   2   0   0   4   0]\n",
      " [  0   1   1  47   2   0   0   1   0]\n",
      " [  1   4   1   1  99   1   7   8   1]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  0   1   0   0   4   0  86   3   0]\n",
      " [  0   4   2   1  14   0   2 102   1]\n",
      " [  0   0   0   0   0   0   0   0  12]]\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/5\n",
      "38/64 [================>.............] - ETA: 1s - loss: 1.8857 - accuracy: 0.2471"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-95d42a3c008e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_lstm_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/Environments/py376/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "fold = 0\n",
    "accuracies = []\n",
    "for train_index, test_index in cv.split(X):\n",
    "    fold += 1\n",
    "    X_train, X_test = X_lstm[train_index], X_lstm[test_index]\n",
    "    y_train, y_test = y_nn[train_index], y_nn[test_index]\n",
    "    \n",
    "    model = get_lstm_model()\n",
    "    \n",
    "    history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    fine_pred = [np.argmax(p) for p in predictions]\n",
    "    fine_gt = [np.argmax(p) for p in y_test]\n",
    "\n",
    "    PRC_matrics(fine_pred, fine_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 32.323\n",
      "Recall: 34.409\n",
      "F-Measure: 3333.333\n",
      "Accuracy score: 25.000\n",
      "Confustion matrix: \n",
      "[[28  4  6 10  8 12  2]\n",
      " [ 1  4  7  5  3  2  1]\n",
      " [ 5  2 12 14  9  9  1]\n",
      " [15  5 12 34 28 27  2]\n",
      " [13  3  9 17 26 25  1]\n",
      " [13  4 14 31 20 35  9]\n",
      " [ 1  1  0  1  4  4  1]]\n"
     ]
    }
   ],
   "source": [
    "fine_pred = [np.argmax(p) for p in predictions]\n",
    "fine_gt = [np.argmax(p) for p in y_test]\n",
    "PRC_matrics(fine_pred, fine_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. BONUS - experiment with a BERT-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(dataset['a'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a bert tockenizer\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "os.environ['TFHUB_DOWNLOAD_PROGRESS'] = \"1\"\n",
    "\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = [tokenize_reviews(qu) for qu in dataset['questions'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_with_len = [[question, y[i], len(question)] for i, question in enumerate(tokenized_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews_with_len[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(reviews_with_len)\n",
    "sorted_reviews_labels = [(review_lab[0], review_lab[1]) for review_lab in reviews_with_len]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "processed_dataset = tf.data.Dataset.from_generator(lambda: sorted_reviews_labels, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batched_dataset = processed_dataset.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next(iter(batched_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 10\n",
    "\n",
    "DROPOUT_RATE = 0.2\n",
    "\n",
    "NB_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "#                         embedding_dimensions=EMB_DIM,\n",
    "#                         cnn_filters=CNN_FILTERS,\n",
    "#                         dnn_units=DNN_UNITS,\n",
    "#                         model_output_classes=OUTPUT_CLASSES,\n",
    "#                         dropout_rate=DROPOUT_RATE)\n",
    "\n",
    "def create_and_compile_bert():\n",
    "    text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                            embedding_dimensions=EMB_DIM,\n",
    "                            cnn_filters=CNN_FILTERS,\n",
    "                            dnn_units=DNN_UNITS,\n",
    "                            model_output_classes=OUTPUT_CLASSES,\n",
    "                            dropout_rate=DROPOUT_RATE)\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    return text_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.TEXT_MODEL at 0x7ffa317906d0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_and_compile_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensorflow.python.data.ops.dataset_ops.PaddedBatchDataset"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(batched_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "142/142 [==============================] - 12s 83ms/step - loss: 1.3756 - sparse_categorical_accuracy: 0.4847\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.4538 - sparse_categorical_accuracy: 0.8551\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.1469 - sparse_categorical_accuracy: 0.9613\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0473 - sparse_categorical_accuracy: 0.9903\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0228 - sparse_categorical_accuracy: 0.9960\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0082 - sparse_categorical_accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 4s 32ms/step - loss: 0.0039 - sparse_categorical_accuracy: 0.9991\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0020 - sparse_categorical_accuracy: 0.9996\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.8002 - sparse_categorical_accuracy: 0.8333\n",
      "[0.8001834154129028, 0.8333333134651184]\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 1.4042 - sparse_categorical_accuracy: 0.4843\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.4645 - sparse_categorical_accuracy: 0.8544\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.1424 - sparse_categorical_accuracy: 0.9608\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0448 - sparse_categorical_accuracy: 0.9905\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0189 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9982\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0098 - sparse_categorical_accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9989\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9996\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.8445 - sparse_categorical_accuracy: 0.8188\n",
      "[0.8444827795028687, 0.8187500238418579]\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 1.3931 - sparse_categorical_accuracy: 0.4816\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.4654 - sparse_categorical_accuracy: 0.8529\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.1527 - sparse_categorical_accuracy: 0.9577\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0492 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0206 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9976\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9987\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 5s 36ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9989\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9993\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0027 - sparse_categorical_accuracy: 0.9993\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.7926 - sparse_categorical_accuracy: 0.8292\n",
      "[0.7925731539726257, 0.8291666507720947]\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 1.3694 - sparse_categorical_accuracy: 0.4945\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.4579 - sparse_categorical_accuracy: 0.8573\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.1472 - sparse_categorical_accuracy: 0.9637\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0469 - sparse_categorical_accuracy: 0.9903\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0201 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0129 - sparse_categorical_accuracy: 0.9976\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0067 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 0.0045 - sparse_categorical_accuracy: 0.9991\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9993\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0019 - sparse_categorical_accuracy: 0.9998\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.8478 - sparse_categorical_accuracy: 0.8229\n",
      "[0.8478280901908875, 0.8229166865348816]\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 1.3813 - sparse_categorical_accuracy: 0.4903\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 0.4650 - sparse_categorical_accuracy: 0.8535\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 0.1471 - sparse_categorical_accuracy: 0.9584\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0469 - sparse_categorical_accuracy: 0.9885\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 0.0210 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 0.0116 - sparse_categorical_accuracy: 0.9978\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0078 - sparse_categorical_accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0055 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0028 - sparse_categorical_accuracy: 0.9996\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.8016 - sparse_categorical_accuracy: 0.8167\n",
      "[0.8015813827514648, 0.8166666626930237]\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 1.3926 - sparse_categorical_accuracy: 0.4790\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.4587 - sparse_categorical_accuracy: 0.8562: 0s - loss: 0.4968 - sparse_categorical_accu\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.1448 - sparse_categorical_accuracy: 0.9635\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0429 - sparse_categorical_accuracy: 0.9923\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0191 - sparse_categorical_accuracy: 0.9967\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0146 - sparse_categorical_accuracy: 0.9971: 2s - loss: 0.0163 - \n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0086 - sparse_categorical_accuracy: 0.9978\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0052 - sparse_categorical_accuracy: 0.9989\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0034 - sparse_categorical_accuracy: 0.9993\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 4s 30ms/step - loss: 0.0024 - sparse_categorical_accuracy: 0.9993\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.8524 - sparse_categorical_accuracy: 0.8250\n",
      "[0.8524056673049927, 0.824999988079071]\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 4s 30ms/step - loss: 1.3692 - sparse_categorical_accuracy: 0.4985\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 4s 29ms/step - loss: 0.4606 - sparse_categorical_accuracy: 0.8571\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 4s 30ms/step - loss: 0.1619 - sparse_categorical_accuracy: 0.9562\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 4s 29ms/step - loss: 0.0507 - sparse_categorical_accuracy: 0.9883\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 4s 29ms/step - loss: 0.0186 - sparse_categorical_accuracy: 0.9969\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 4s 29ms/step - loss: 0.0098 - sparse_categorical_accuracy: 0.9980\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 4s 30ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0057 - sparse_categorical_accuracy: 0.9982\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 4s 30ms/step - loss: 0.0043 - sparse_categorical_accuracy: 0.9987\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 4s 30ms/step - loss: 0.0035 - sparse_categorical_accuracy: 0.9991\n",
      "15/15 [==============================] - 0s 7ms/step - loss: 0.8174 - sparse_categorical_accuracy: 0.8229\n",
      "[0.8173688650131226, 0.8229166865348816]\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 1.3651 - sparse_categorical_accuracy: 0.4927\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.4467 - sparse_categorical_accuracy: 0.8613\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 4s 30ms/step - loss: 0.1449 - sparse_categorical_accuracy: 0.9602\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 4s 30ms/step - loss: 0.0463 - sparse_categorical_accuracy: 0.9889\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0190 - sparse_categorical_accuracy: 0.9956\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0107 - sparse_categorical_accuracy: 0.9980\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0114 - sparse_categorical_accuracy: 0.9980\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0058 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 4s 31ms/step - loss: 0.0050 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0031 - sparse_categorical_accuracy: 0.9993\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.8388 - sparse_categorical_accuracy: 0.8167\n",
      "[0.8387882113456726, 0.8166666626930237]\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 1.3923 - sparse_categorical_accuracy: 0.4772\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.4627 - sparse_categorical_accuracy: 0.8540\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.1490 - sparse_categorical_accuracy: 0.9611\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 5s 36ms/step - loss: 0.0468 - sparse_categorical_accuracy: 0.9903\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 0.0212 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 0.0105 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0066 - sparse_categorical_accuracy: 0.9982\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 0.0061 - sparse_categorical_accuracy: 0.9987\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0033 - sparse_categorical_accuracy: 0.9993\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0022 - sparse_categorical_accuracy: 0.9996\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 0.7865 - sparse_categorical_accuracy: 0.8313\n",
      "[0.7865497469902039, 0.831250011920929]\n",
      "Epoch 1/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 1.3783 - sparse_categorical_accuracy: 0.4834\n",
      "Epoch 2/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.4668 - sparse_categorical_accuracy: 0.8502\n",
      "Epoch 3/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.1495 - sparse_categorical_accuracy: 0.9584\n",
      "Epoch 4/10\n",
      "142/142 [==============================] - 5s 32ms/step - loss: 0.0490 - sparse_categorical_accuracy: 0.9900\n",
      "Epoch 5/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0214 - sparse_categorical_accuracy: 0.9965\n",
      "Epoch 6/10\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 0.0143 - sparse_categorical_accuracy: 0.9973\n",
      "Epoch 7/10\n",
      "142/142 [==============================] - 5s 34ms/step - loss: 0.0069 - sparse_categorical_accuracy: 0.9989\n",
      "Epoch 8/10\n",
      "142/142 [==============================] - 5s 33ms/step - loss: 0.0063 - sparse_categorical_accuracy: 0.9985\n",
      "Epoch 9/10\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 0.0040 - sparse_categorical_accuracy: 0.9991\n",
      "Epoch 10/10\n",
      "142/142 [==============================] - 5s 35ms/step - loss: 0.0018 - sparse_categorical_accuracy: 1.0000\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 0.8625 - sparse_categorical_accuracy: 0.8062\n",
      "[0.8624716401100159, 0.8062499761581421]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import math\n",
    "TOTAL_BATCHES = math.ceil(len(sorted_reviews_labels) / BATCH_SIZE)\n",
    "TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "    \n",
    "cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "fold = 0\n",
    "accuracies = []\n",
    "for i in range(10):\n",
    "    fold += 1\n",
    "    \n",
    "    text_model = create_and_compile_bert()\n",
    "    \n",
    "    # shuffel and take 10 batches\n",
    "    batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "    test_data = batched_dataset.take(TEST_BATCHES)\n",
    "    train_data = batched_dataset.skip(TEST_BATCHES)\n",
    "    \n",
    "\n",
    "    text_model.fit(train_data, epochs=NB_EPOCHS)\n",
    "    results = text_model.evaluate(test_data)\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-65-c68ce269e8a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for array"
     ]
    }
   ],
   "source": [
    "np.array(test_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6402062e-05, 3.2555621e-05, 1.3065638e-06, 5.3962822e-07,\n",
       "       4.1490410e-02, 3.2934950e-05, 9.5815974e-01, 2.6502833e-04,\n",
       "       1.5510790e-07, 9.8319958e-07], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = text_model.predict(test_data)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_pred = [np.argmax(p) for p in predictions]\n",
    "bert_gt = [np.argmax(p) for p in y_test]\n",
    "PRC_matrics(bert_pred, bert_gt)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
