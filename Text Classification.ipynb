{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import some of the main packages and set the memory limit of the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "assert len(physical_devices) > 0, \"Not enough GPU hardware devices available\"\n",
    "# config = tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.set_virtual_device_configuration(physical_devices[0], [\n",
    "tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2000)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data\n",
    "\n",
    "Load the CSV dataset using pandas library and remove Nan values if there is any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 3 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   questions  5000 non-null   object\n",
      " 1   a          5000 non-null   object\n",
      " 2   b          5000 non-null   object\n",
      "dtypes: object(3)\n",
      "memory usage: 117.3+ KB\n",
      "None\n",
      "['TTD' 'TGU' 'ACM' 'TRS' 'WTH' 'FOD' 'ENT' 'TGU\\n' 'TTD\\n' '\\nENT']\n",
      "79\n",
      "What are the companies which organize shark feeding events for scuba divers?\n"
     ]
    }
   ],
   "source": [
    "file_name = 'data/Travel-Dataset-5000--master/5000TravelQuestionsDataset.xlsx'\n",
    "test_id  = 1\n",
    "col_names = ['questions', 'a', 'b']\n",
    "dataset = pd.read_excel(file_name, header=None, names=col_names)\n",
    "dataset['questions'].dropna(inplace=True)\n",
    "print(dataset.info())\n",
    "print(dataset['a'].unique())\n",
    "print(len(dataset['b'].unique()))\n",
    "print(dataset['questions'][test_id])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PreProcessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do basic preprocessing\n",
    "- Removing HTML tags\n",
    "- Removing punctuation\n",
    "- Lowering text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are the companies which organize shark feeding events for scuba divers '"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    Applies some pre-processing on the given text.\n",
    "\n",
    "    Steps :\n",
    "    - Removing HTML tags\n",
    "    - Removing punctuation\n",
    "    - Lowering text\n",
    "    \"\"\"\n",
    "    \n",
    "    # remove HTML tags\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # remove the characters [\\], ['] and [\"]\n",
    "    text = re.sub(r\"\\\\\", \"\", text)    \n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)    \n",
    "    \n",
    "    # convert text to lowercase\n",
    "    text = text.strip().lower()\n",
    "    \n",
    "    # replace punctuation characters with spaces\n",
    "    filters='!\"\\'#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n'\n",
    "    translate_dict = dict((c, \" \") for c in filters)\n",
    "    translate_map = str.maketrans(translate_dict)\n",
    "    text = text.translate(translate_map)\n",
    "\n",
    "    return text\n",
    "\n",
    "dataset['clean_questions'] = [clean_text(question) for question in dataset['questions']]\n",
    "dataset['clean_questions'][test_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def remove_punctuation(x):\n",
    "    return \"\".join([char for char in x if char not in string.punctuation])\n",
    "\n",
    "# dataset['questions'] = [remove_punctuation(question) for question in dataset['questions']]\n",
    "# dataset['questions'][test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove stop words other than the first word then remove punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the companies which organize shark feeding events for scuba divers?\n",
      "What companies organize shark feeding events scuba divers \n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def remove_stopwords(x):\n",
    "    words = word_tokenize(x)\n",
    "    return \" \".join([word for pos, word in enumerate(words) if (pos < 1) or (word not in stop_words)])\n",
    "\n",
    "dataset['questions_stop'] = [remove_punctuation(remove_stopwords(question)) for question in dataset['questions']]\n",
    "print(dataset['questions'][test_id])\n",
    "print(dataset['questions_stop'][test_id])\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize and create BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'what are the company which organize shark feeding event for scuba diver'"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "  \n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize(x):\n",
    "    filtered_words = nltk.word_tokenize(x)\n",
    "    lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "dataset['lem_questions'] = [lemmatize(question) for question in dataset['clean_questions']]\n",
    "dataset['lem_questions'][test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## POS Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'WP VBP DT NNS WDT VBP NN NN NNS IN NN NNS .'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "def pos_tagging(x):\n",
    "    words = nltk.word_tokenize(x)\n",
    "    lst = [ r[1] for r in pos_tag(words)] \n",
    "    return ' '.join(lst)\n",
    "\n",
    "dataset['pos_questions'] = [pos_tagging(question) for question in dataset['questions']]\n",
    "dataset['pos_questions'][test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naming Entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dulan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree\n",
    "\n",
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    if continuous_chunk:\n",
    "        named_entity = \" \".join(current_chunk)\n",
    "        if named_entity not in continuous_chunk:\n",
    "            continuous_chunk.append(named_entity)\n",
    "    \n",
    "    def remove_null(x):\n",
    "        if '' in x:\n",
    "            x.remove('')\n",
    "        return x\n",
    "\n",
    "    lst = remove_null(continuous_chunk)\n",
    "    return ' '.join(lst)\n",
    "\n",
    "txt = \"Barack Obama is a great person.\" \n",
    "txt2 = \"Who is Dulan?\"\n",
    "print (get_continuous_chunks(txt2))\n",
    "\n",
    "\n",
    "\n",
    "dataset['ne_questions'] = [get_continuous_chunks(question) for question in dataset['questions']]\n",
    "dataset['ne_questions'][test_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count vectorizer (BOW)\n",
    "Remove stop words and vectorize the content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 1177)\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def get_count_vect(documents):\n",
    "    vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7)#, stop_words=stopwords.words('english')\n",
    "    X = vectorizer.fit_transform(documents).toarray()\n",
    "    print(X.shape)\n",
    "    return X\n",
    "\n",
    "print(get_count_vect(dataset['questions']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head word feature\n",
    "Extract the head word from the coupus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Head word tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "def head_word_tokenizer(text):\n",
    "    head_words = []\n",
    "    for token in nlp(text):\n",
    "        if token.dep_ == \"nsubj\" or token.dep_ == \"nsubjpass\":\n",
    "            head_words.append(token.text)\n",
    "#             head_words.append(token.head.text)\n",
    "    return head_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "head_word_tokenizer(\"big red dog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "head_words_vectorizer = CountVectorizer(tokenizer = head_word_tokenizer,max_features=100,stop_words=stopwords.words('english'))\n",
    "head_words_vector = head_words_vectorizer.fit_transform(dataset[\"questions\"].values).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Head word Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet \n",
    "\n",
    "def get_syonyms(words):\n",
    "    all_synonyms = []\n",
    "    for word in words.split(' '):\n",
    "        synonyms = []\n",
    "\n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "\n",
    "        synonyms = list(set(synonyms))\n",
    "        all_synonyms += synonyms\n",
    "        \n",
    "    return all_synonyms\n",
    "\n",
    "# print(get_syonyms(\"cat and dog\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:385: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'tween\", '1', '1000', '500', 'A', 'AM', 'AN', 'AS', 'Am', 'American_Samoa', 'Artium_Magister', 'As', 'Associate_in_Nursing', 'At', 'Bay_State', 'Be', 'Beaver_State', 'D', 'DO', 'DOE', 'Department_of_Energy', 'Doctor_of_Osteopathy', 'Don', 'Don_River', 'Down', 'Eastern_Samoa', 'Energy', 'Energy_Department', 'Evergreen_State', 'G', 'HA', 'He', 'Hera', 'Here', 'Hoosier_State', 'I', 'IN', 'ISN', 'IT', 'In', 'Indiana', 'International_Relations_and_Security_Network', 'John_L._H._Down', 'K', \"KO'd\", 'Lapp', 'Lapplander', 'M', 'MA', 'MB', 'ME', 'MT', 'Maine', 'Massachusetts', 'Master_of_Arts', 'MiB', 'More', 'No', 'North_Korean_won', 'O', 'OR', 'Old_Colony', 'Oregon', 'Pine_Tree_State', 'Ra', 'Re', 'S', 'Saame', 'Saami', 'Same', 'Sami', 'Shan', 'Sir_Thomas_More', 'South_Korean_won', 'T', 'Tai_Long', 'Thomas_More', 'WA', 'WHO', 'Washington', 'World_Health_Organization', 'Y', 'accept', 'ace', 'acquire', 'act', 'adenine', 'advance', 'afterward', 'afterwards', 'ahead', 'all_over', 'almost', 'alone', 'along', 'also', 'altogether', 'americium', 'amp', 'ampere', 'amplitude_modulation', 'and_so', 'and_then', 'ane', 'angstrom', 'angstrom_unit', 'answer', 'antiophthalmic_factor', 'apiece', 'approximately', 'ar', 'around', 'arrange', 'arse', 'arsenic', 'as_well', 'aside', 'ass', 'assume', 'astatine', 'astir', 'at_a_lower_place', 'at_once', 'at_one_time', 'at_present', 'at_that_place', 'atomic_number_102', 'atomic_number_16', 'atomic_number_2', 'atomic_number_33', 'atomic_number_39', 'atomic_number_4', 'atomic_number_49', 'atomic_number_53', 'atomic_number_75', 'atomic_number_8', 'atomic_number_85', 'atomic_number_95', 'away', 'axerophthol', 'backside', 'barely', 'bash', 'bathroom', 'bear', 'behave', 'behind', 'beingness', 'belt_down', 'beneath', 'bequeath', 'beryllium', 'besides', 'betwixt', 'birth', 'blue', 'bolt_down', 'boost', 'bottom', 'brawl', 'bring_home_the_bacon', 'bum', 'bump_off', 'buns', 'butt', 'buttocks', 'calciferol', 'can_buoy', 'cancelled', 'canful', 'cause', 'chiliad', 'cholecalciferol', 'close_to', 'coif', 'coiffe', 'coiffure', 'come', 'come_out', 'come_out_of_the_closet', 'come_through', 'commode', 'complete', 'completely', 'comprise', 'concluded', 'constitute', 'consume', 'cost', 'crapper', 'cut_down', 'deliver', 'deliver_the_goods', 'deoxyadenosine_monophosphate', 'deoxythymidine_monophosphate', 'depressed', 'derriere', 'devour', 'directly', 'dismiss', 'dispatch', 'dispirited', 'displace', 'doe', 'doh', 'done', 'down_feather', 'down_in_the_mouth', 'down_pat', 'down_the_stairs', 'downcast', 'downhearted', 'downstairs', 'downward', 'downwardly', 'downwards', 'dress', 'drink_down', 'due_south', 'earlier', 'early', 'embody', 'encourage', 'ended', 'entirely', 'entropy', 'equal', 'equally', 'equitable', 'ergocalciferol', 'erst', 'erstwhile', 'every_bit', 'exactly', 'excessively', 'exclusively', 'execute', 'exercise', 'exist', 'existence', 'experience', 'extinct', 'fair', 'fanny', 'far', 'fare', 'farther', 'father', 'feature', 'fine-tune', 'fire', 'five_hundred', 'follow', 'for_each_one', 'forbidden', 'force_out', 'former', 'formerly', 'forth', 'forthwith', 'foster', 'from_each_one', 'fundament', 'gain', 'gain_ground', 'get', 'get_ahead', 'get_along', 'get_into', 'give', 'give_birth', 'give_notice', 'give_the_axe', 'give_the_sack', 'gloomy', 'glucinium', 'go_through', 'good', 'grand', 'grim', 'group_A', 'group_O', 'harbor', 'harbour', 'hardly', 'have_got', 'helium', 'hence', 'higher_up', 'hind_end', 'hindquarters', 'hit', 'hither', 'hold', 'hour_angle', 'identical', 'immediately', 'improving', 'in_a_higher_place', 'in_front', 'in_one_case', 'in_that_location', 'in_that_respect', 'inch', 'indeed', 'indium', 'induce', 'information_technology', 'infra', 'ingest', 'instantly', 'inward', 'inwards', 'iodin', 'iodine', 'john', 'just_about', 'just_now', 'k', 'kayoed', 'keister', 'kill', 'knock_down', 'knocked_out', 'land', 'later', 'later_on', 'lav', 'lavatory', 'leave', 'let', 'like', 'like_a_shot', 'likewise', 'liothyronine', 'live', 'lone', 'lonesome', 'low', 'low-spirited', 'mA', 'make', 'make_headway', 'make_out', 'make_up', 'mama', 'mamma', 'mammy', 'manage', 'mastered', 'mebibyte', 'megabyte', 'merely', 'meter', 'metre', 'metric_ton', 'mho', 'milliampere', 'molar_concentration', 'molarity', 'mom', 'momma', 'mommy', 'more_or_less', 'more_than', 'mum', 'mummy', 'murder', 'nates', 'near', 'nearly', 'nether', 'nigh', 'no_more', 'nobelium', 'non', 'nowadays', \"o'er\", 'oasis', 'on_a_lower_floor', 'on_that_point', 'once_again', 'once_more', 'one', 'one_thousand', 'one_time', 'only_if', 'only_when', 'operating_room', 'operating_theater', 'operating_theatre', 'or_so', 'organism', 'over_again', 'overly', 'oxygen', 'past', 'patch', 'perform', 'personify', 'piece', 'pile', 'polish', 'polish_off', 'pop', 'possess', 'posterior', 'pot', 'potty', 'pour_down', 'practice', 'practise', 'prat', 'preceptor', 'precisely', 'privy', 'prohibited', 'promote', 'proscribed', 'pull_ahead', 'pull_down', 'push_down', 'put_on', 'put_up', 'randomness', 'rattling', 'ray', 'real', 'really', 'rear', 'rear_end', 'receive', 'reciprocal_ohm', 'refine', 'remove', 'represent', 'rhenium', 'rich_person', 'right_away', 'roughly', 'rump', 'sack', 'scarce', 'scarcely', 'seaport', 'seat', 'sec', 'second', 'selfsame', 'send_away', 'serve', 'set', 'shoot_down', 'siemens', 'simply', 'single', 'slay', 'soh', 'sol', 'sole', 'solely', 'solitary', 'sour', 'south', 'southward', 'spell', 'stern', 'stimulate', 'stool', 'straight_off', 'straightaway', 'stunned', 'subsequently', 'succeed', 'suffer', 'suffice', 'sulfur', 'sulphur', 'supra', 'surgery', 'sustain', 'taboo', 'tabu', 'tail', 'tail_end', 'take', 'take_in', 'terminate', 'terminated', 'testament', 'tetraiodothyronine', 'thence', 'therefore', 'thither', 'thou', 'thousand', 'throne', 'through_and_through', 'through_with', 'throw', 'thus', 'thusly', 'thymine', 'thyroxin', 'thyroxine', 'tin', 'tin_can', 'to_a_fault', 'to_a_greater_extent', 'to_a_higher_place', 'to_a_lower_place', 'to_each_one', 'to_the_highest_degree', 'today', 'toilet', 'tonne', 'tooshie', 'toss_off', 'totally', 'triiodothyronine', 'turned', 'tush', 'type_A', 'type_O', 'unity', 'upright', 'upward', 'upwardly', 'upwards', 'ut', 'verboten', 'viosterol', 'virtually', 'vitamin_A', 'vitamin_D', 'volition', 'wealthy_person', 'wear', 'well-nigh', 'whatever', 'whatsoever', 'wherefore', 'whole', 'wholly', 'win', 'wye', 'yard', 'yttrium'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "head_words_synonym_vectorizer = CountVectorizer(tokenizer = get_syonyms,max_features=100,stop_words=stopwords.words('english'))\n",
    "head_words_synonym_vector = head_words_synonym_vectorizer.fit_transform(dataset[\"questions\"].values).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are the companies which organize shark feeding events for scuba divers?\n",
      "Whatcompanies companiesorganize organizeshark sharkfeeding feedingevents eventsscuba scubadivers\n"
     ]
    }
   ],
   "source": [
    "def bigram(x):\n",
    "    new_words = \"\"\n",
    "    pre_word = None\n",
    "    for word in x.strip().split(' '):\n",
    "        \n",
    "        if pre_word is not None:\n",
    "            new_words += \"{}{} \".format(pre_word, word)\n",
    "        pre_word = word\n",
    "    return new_words[:-1]\n",
    "\n",
    "# print(bigram(\"what are the company which organize shark feeding event for scuba divers \"))\n",
    "\n",
    "dataset['questions_bigram'] = [bigram(question) for question in dataset['questions_stop']]\n",
    "\n",
    "print(dataset['questions'][test_id])\n",
    "print(dataset['questions_bigram'][test_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# head_words_vectorizer = CountVectorizer(tokenizer = bigram,max_features=10000)\n",
    "# head_words_vector = head_words_vectorizer.fit_transform(dataset[\"questions\"].values).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy Evauluation on Different Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "def PRC_matrics(y_test, prediction):\n",
    "    # calculate prediction\n",
    "    precision = precision_score(y_test, prediction, labels=[1,2], average='micro')*100\n",
    "    print('Precision: %.3f' % precision)\n",
    "\n",
    "    # calculate recall\n",
    "    recall = recall_score(y_test, prediction, labels=[1,2], average='micro')*100\n",
    "    print('Recall: %.3f' % recall)\n",
    "    \n",
    "    # calculate score\n",
    "#     score = f1_score(y_test, prediction, average='micro')\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    print('F-Measure: %.3f' % f1_score)\n",
    "    \n",
    "    \n",
    "    acc = accuracy_score(y_test, prediction)*100\n",
    "#     tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "#     acc2 = (tn + tp)*100/(tn + fp + fn + tp)\n",
    "    print('Accuracy score: %.3f' % acc)\n",
    "    \n",
    "    \n",
    "    cm = confusion_matrix(y_test, prediction)\n",
    "    print(\"\\nConfustion matrix: \\n{}\".format(cm))\n",
    "    \n",
    "    return precision, recall, f1_score, acc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the y values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_encoded_y(feature_set='a'):\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(dataset[feature_set])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default SVM training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def train_with_svm(XX, y):\n",
    "    best_prediction = None\n",
    "    best_test = None\n",
    "    best_accuracy = 0\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    fold = 0\n",
    "    accuracies = []\n",
    "    for train_index, test_index in cv.split(XX):\n",
    "        fold += 1\n",
    "        X_train, X_test = XX[train_index], XX[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        SVM = SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "        SVM.fit(X_train,y_train)\n",
    "        predictions_SVM1 = SVM.predict(X_test)\n",
    "        acc = accuracy_score(predictions_SVM1, y_test)*100\n",
    "        if best_accuracy < acc:\n",
    "            best_accuracy = acc\n",
    "            best_prediction = predictions_SVM1\n",
    "            best_test = y_test\n",
    "            best_model = SVM\n",
    "        accuracies.append(acc)\n",
    "        print(\"Fold - {} - {} - {:.2f}\".format(fold, \"SVM Accuracy Score -> \",acc))\n",
    "\n",
    "    print(\"Mean Accuracy {:.2f} \\nStd Accuracy {:.2f}\\n\\n\".format(np.mean(accuracies), np.std(accuracies)))\n",
    "    \n",
    "    print(\"Best accuracy : {}\".format(best_accuracy))\n",
    "    PRC_matrics(best_test, best_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM - Normal train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1 - A traditional ML classifier s.a. SVM or Logistic Regression with at least 5  of the features mentioned in the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Used features\n",
    "\n",
    "+ ST(BOW)\n",
    "+ NE(BOW)\n",
    "+ POS(BOW)\n",
    "+ Head Word\n",
    "+ Head Word Synonyms\n",
    "+ ST(BG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 27)\n",
      "(5000, 326)\n",
      "(5000, 527)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(5000, 6080)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from scipy.sparse import coo_matrix, csr_matrix, hstack\n",
    "\n",
    "\n",
    "y = get_encoded_y('a')\n",
    "\n",
    "X_lem = dataset['lem_questions']\n",
    "tfidf_lem = TfidfVectorizer(max_features=5000)\n",
    "tfidf_lem.fit(X_lem)\n",
    "\n",
    "X_pos = dataset['pos_questions']\n",
    "\n",
    "X_ne = dataset['ne_questions']\n",
    "\n",
    "X_bigram = dataset['questions_bigram']\n",
    "\n",
    "XX = csr_matrix(hstack([tfidf_lem.transform(X_lem) ,get_count_vect(X_pos), get_count_vect(X_ne), get_count_vect(X_bigram),\n",
    "                        head_words_vector, head_words_synonym_vector]))\n",
    "XX_simple = csr_matrix(hstack([tfidf_lem.transform(X_lem)]))\n",
    "XX.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Select K best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 3000)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "\n",
    "X_new = SelectKBest(chi2, k=3000).fit_transform(XX, y)\n",
    "X_new.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Remove if variaence less than a specific threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 153)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "X_new_var = VarianceThreshold(threshold=(0.01)).fit_transform(XX)\n",
    "X_new_var.shape\n",
    "# selector.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only using ST(BOW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 83.20\n",
      "Fold - 2 - SVM Accuracy Score ->  - 84.80\n",
      "Fold - 3 - SVM Accuracy Score ->  - 81.40\n",
      "Fold - 4 - SVM Accuracy Score ->  - 81.80\n",
      "Fold - 5 - SVM Accuracy Score ->  - 83.40\n",
      "Fold - 6 - SVM Accuracy Score ->  - 84.20\n",
      "Fold - 7 - SVM Accuracy Score ->  - 83.40\n",
      "Fold - 8 - SVM Accuracy Score ->  - 84.40\n",
      "Fold - 9 - SVM Accuracy Score ->  - 82.80\n",
      "Fold - 10 - SVM Accuracy Score ->  - 82.80\n",
      "Mean Accuracy 83.22 \n",
      "Std Accuracy 1.03\n",
      "\n",
      "\n",
      "Best accuracy : 84.8\n",
      "Precision: 91.304\n",
      "Recall: 78.505\n",
      "F-Measure: 84.422\n",
      "Accuracy score: 84.800\n",
      "\n",
      "Confustion matrix: \n",
      "[[ 72   0   0   5   1   4   0   0]\n",
      " [  0  12   0   4   0   9   0   0]\n",
      " [  2   0  35   0   0   2   0   0]\n",
      " [  4   0   1 108   5   6   0   0]\n",
      " [  0   0   0  10  81   3   0   0]\n",
      " [  2   0   1   6   2  99   0   0]\n",
      " [  0   0   0   0   0   1   0   0]\n",
      " [  0   0   0   6   0   2   0  17]]\n"
     ]
    }
   ],
   "source": [
    "train_with_svm(XX_simple, get_encoded_y('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 65.40\n",
      "Fold - 2 - SVM Accuracy Score ->  - 65.00\n",
      "Fold - 3 - SVM Accuracy Score ->  - 64.00\n",
      "Fold - 4 - SVM Accuracy Score ->  - 64.40\n",
      "Fold - 5 - SVM Accuracy Score ->  - 65.80\n",
      "Fold - 6 - SVM Accuracy Score ->  - 66.00\n",
      "Fold - 7 - SVM Accuracy Score ->  - 63.20\n",
      "Fold - 8 - SVM Accuracy Score ->  - 67.80\n",
      "Fold - 9 - SVM Accuracy Score ->  - 64.80\n",
      "Fold - 10 - SVM Accuracy Score ->  - 61.60\n",
      "Mean Accuracy 64.80 \n",
      "Std Accuracy 1.59\n",
      "\n",
      "\n",
      "Best accuracy : 67.80000000000001\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 67.800\n",
      "\n",
      "Confustion matrix: \n",
      "[[ 0  0  0 ...  0  0  0]\n",
      " [ 0  5  0 ...  0  0  0]\n",
      " [ 0  0 26 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "train_with_svm(XX_simple, get_encoded_y('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM - Train with all the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 75.80\n",
      "Fold - 2 - SVM Accuracy Score ->  - 80.20\n",
      "Fold - 3 - SVM Accuracy Score ->  - 77.80\n",
      "Fold - 4 - SVM Accuracy Score ->  - 76.00\n",
      "Fold - 5 - SVM Accuracy Score ->  - 77.40\n",
      "Fold - 6 - SVM Accuracy Score ->  - 81.60\n",
      "Fold - 7 - SVM Accuracy Score ->  - 78.20\n",
      "Fold - 8 - SVM Accuracy Score ->  - 77.60\n",
      "Fold - 9 - SVM Accuracy Score ->  - 76.00\n",
      "Fold - 10 - SVM Accuracy Score ->  - 75.80\n",
      "Mean Accuracy 77.64 \n",
      "Std Accuracy 1.87\n",
      "\n",
      "\n",
      "Best accuracy : 81.6\n",
      "Precision: 77.215\n",
      "Recall: 83.562\n",
      "F-Measure: 80.263\n",
      "Accuracy score: 81.600\n",
      "\n",
      "Confustion matrix: \n",
      "[[ 53   0   0   0   0   6   0]\n",
      " [  1   8   0   0   0   5   0]\n",
      " [  5   0  40   0   1   4   0]\n",
      " [  5   0   5 111   8  19   0]\n",
      " [  2   1   0   8  94   4   1]\n",
      " [  2   2   1   9   1  86   0]\n",
      " [  0   0   0   1   1   0  16]]\n"
     ]
    }
   ],
   "source": [
    "train_with_svm(XX, get_encoded_y('a'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 48.00\n",
      "Fold - 2 - SVM Accuracy Score ->  - 50.00\n",
      "Fold - 3 - SVM Accuracy Score ->  - 49.80\n",
      "Fold - 4 - SVM Accuracy Score ->  - 49.20\n",
      "Fold - 5 - SVM Accuracy Score ->  - 48.00\n",
      "Fold - 6 - SVM Accuracy Score ->  - 47.00\n",
      "Fold - 7 - SVM Accuracy Score ->  - 44.60\n",
      "Fold - 8 - SVM Accuracy Score ->  - 48.20\n",
      "Fold - 9 - SVM Accuracy Score ->  - 50.40\n",
      "Fold - 10 - SVM Accuracy Score ->  - 46.60\n",
      "Mean Accuracy 48.18 \n",
      "Std Accuracy 1.69\n",
      "\n",
      "\n",
      "Best accuracy : 50.4\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 50.400\n",
      "\n",
      "Confustion matrix: \n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 4 0 1]\n",
      " [0 0 0 ... 3 0 0]\n",
      " [0 0 0 ... 1 0 1]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "train_with_svm(XX, get_encoded_y('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim Doc2Vec - Using Sentence Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "import smart_open\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "def read_corpus(fname, tokens_only=False):\n",
    "    with smart_open.open(fname, encoding=\"iso-8859-1\") as f:\n",
    "        for i, line in enumerate(f):\n",
    "            tokens = simple_preprocess(line)\n",
    "            if tokens_only:\n",
    "                yield tokens\n",
    "            else:\n",
    "                # For training data, add tags\n",
    "                yield TaggedDocument(tokens, [i])\n",
    "lee_train_file = 'questions.txt'\n",
    "train_corpus = list(read_corpus(lee_train_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(dataset['questions'])]\n",
    "\n",
    "model = Doc2Vec(vector_size=500, min_count=2, epochs=40)\n",
    "# model = Doc2Vec(documents, vector_size=1000, window=2, min_count=1, workers=4)\n",
    "# train_corpus = dataset['questions'].values\n",
    "\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "def doc2vec(x):\n",
    "    return np.array(model.infer_vector(x.split(' ')))\n",
    "\n",
    "X_doc2vec = np.array([doc2vec(question) for question in dataset['questions']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec('hello world').shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Accuracy Coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 49.80\n",
      "Fold - 2 - SVM Accuracy Score ->  - 53.60\n",
      "Fold - 3 - SVM Accuracy Score ->  - 51.60\n",
      "Fold - 4 - SVM Accuracy Score ->  - 52.40\n",
      "Fold - 5 - SVM Accuracy Score ->  - 51.60\n",
      "Fold - 6 - SVM Accuracy Score ->  - 55.80\n",
      "Fold - 7 - SVM Accuracy Score ->  - 52.20\n",
      "Fold - 8 - SVM Accuracy Score ->  - 49.00\n",
      "Fold - 9 - SVM Accuracy Score ->  - 52.60\n",
      "Fold - 10 - SVM Accuracy Score ->  - 50.20\n",
      "Mean Accuracy 51.88 \n",
      "Std Accuracy 1.87\n",
      "\n",
      "\n",
      "Best accuracy : 55.800000000000004\n",
      "Precision: 52.381\n",
      "Recall: 45.205\n",
      "F-Measure: 48.529\n",
      "Accuracy score: 55.800\n",
      "\n",
      "Confustion matrix: \n",
      "[[33  0  5  6  4 10  1]\n",
      " [ 1  0  3  5  0  5  0]\n",
      " [ 2  1 23  9  2 13  0]\n",
      " [10  0  6 81 22 29  0]\n",
      " [ 6  0  2 21 68 13  0]\n",
      " [ 9  1  3 17  7 64  0]\n",
      " [ 0  0  0  3  1  4 10]]\n"
     ]
    }
   ],
   "source": [
    "train_with_svm(X_doc2vec, get_encoded_y('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Accuracy Fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 31.60\n",
      "Fold - 2 - SVM Accuracy Score ->  - 35.40\n",
      "Fold - 3 - SVM Accuracy Score ->  - 30.00\n",
      "Fold - 4 - SVM Accuracy Score ->  - 31.20\n",
      "Fold - 5 - SVM Accuracy Score ->  - 26.40\n",
      "Fold - 6 - SVM Accuracy Score ->  - 30.60\n",
      "Fold - 7 - SVM Accuracy Score ->  - 31.80\n",
      "Fold - 8 - SVM Accuracy Score ->  - 27.00\n",
      "Fold - 9 - SVM Accuracy Score ->  - 31.00\n",
      "Fold - 10 - SVM Accuracy Score ->  - 31.60\n",
      "Mean Accuracy 30.66 \n",
      "Std Accuracy 2.41\n",
      "\n",
      "\n",
      "Best accuracy : 35.4\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 35.400\n",
      "\n",
      "Confustion matrix: \n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "train_with_svm(X_doc2vec, get_encoded_y('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train fast text train and test model - this didnt give good accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"fasttext_skipgram_travel_questions.bin\" - model loaded\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import os\n",
    "model_name='fasttext_skipgram_travel_questions.bin'\n",
    "\n",
    "if not os.path.exists(model_name):\n",
    "    dataset['questions'].to_csv('questions.txt', sep='.', header=False, index=False)\n",
    "    model_fasttext = fasttext.train_unsupervised('questions.txt', model='skipgram')\n",
    "    model_fasttext.save_model(model_name)\n",
    "    print(\"Model saved as {}\".format(model_name))\n",
    "else:\n",
    "    print(\"\\\"{}\\\" - model loaded\".format(model_name))\n",
    "    model_fasttext = fasttext.load_model(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_fasttext.words\n",
    "len(model_fasttext.get_word_vector(\"the\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_text_mean_transform(X):\n",
    "    words = X.split(' ')    \n",
    "    return np.mean([model_fasttext.get_word_vector(w) for w in words if w in model_fasttext.words]\n",
    "                    or [np.zeros(100)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast_text_first_x_words(X, length):\n",
    "    from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "    X_lstm = pad_sequences(X, maxlen=length)\n",
    "    \n",
    "    words = X.split(' ')    \n",
    "    return np.mean([ft.get_word_vector(w) for w in words if w in model_fasttext.words]\n",
    "                    or [np.zeros(100)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000,)"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['doc2fast_questions'] = [fast_text_mean_transform(question) for question in dataset['questions']]\n",
    "dataset['doc2fast_questions'][test_id]\n",
    "dataset['doc2fast_questions'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 21.20\n",
      "Fold - 2 - SVM Accuracy Score ->  - 24.80\n",
      "Fold - 3 - SVM Accuracy Score ->  - 24.80\n",
      "Fold - 4 - SVM Accuracy Score ->  - 24.80\n",
      "Fold - 5 - SVM Accuracy Score ->  - 25.00\n",
      "Fold - 6 - SVM Accuracy Score ->  - 29.60\n",
      "Fold - 7 - SVM Accuracy Score ->  - 22.00\n",
      "Fold - 8 - SVM Accuracy Score ->  - 23.40\n",
      "Fold - 9 - SVM Accuracy Score ->  - 25.20\n",
      "Fold - 10 - SVM Accuracy Score ->  - 22.40\n",
      "Mean Accuracy 24.32 \n",
      "Std Accuracy 2.22\n",
      "\n",
      "\n",
      "Best accuracy : 29.599999999999998\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 29.600\n",
      "\n",
      "Confustion matrix: \n",
      "[[  0   0   0  59   0   0   0]\n",
      " [  0   0   0  14   0   0   0]\n",
      " [  0   0   0  50   0   0   0]\n",
      " [  0   0   0 148   0   0   0]\n",
      " [  0   0   0 110   0   0   0]\n",
      " [  0   0   0 101   0   0   0]\n",
      " [  0   0   0  18   0   0   0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "X_doc2fast = np.array([fast_text_mean_transform(question) for question in dataset['questions']])\n",
    "le = LabelEncoder()\n",
    "y_doc2fast = le.fit_transform(dataset['a'])\n",
    "\n",
    "train_with_svm(X_doc2fast, get_encoded_y('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download already trained model - this gave better results compaired to the previous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext.util\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_sentence(sentence):\n",
    "    \n",
    "    embedding=ft.get_sentence_vector(sentence)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Accuracy Coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 72.80\n",
      "Fold - 2 - SVM Accuracy Score ->  - 75.40\n",
      "Fold - 3 - SVM Accuracy Score ->  - 72.60\n",
      "Fold - 4 - SVM Accuracy Score ->  - 72.60\n",
      "Fold - 5 - SVM Accuracy Score ->  - 74.60\n",
      "Fold - 6 - SVM Accuracy Score ->  - 75.40\n",
      "Fold - 7 - SVM Accuracy Score ->  - 71.40\n",
      "Fold - 8 - SVM Accuracy Score ->  - 71.80\n",
      "Fold - 9 - SVM Accuracy Score ->  - 76.60\n",
      "Fold - 10 - SVM Accuracy Score ->  - 75.00\n",
      "Mean Accuracy 73.82 \n",
      "Std Accuracy 1.69\n",
      "\n",
      "\n",
      "Best accuracy : 76.6\n",
      "Precision: 85.484\n",
      "Recall: 60.227\n",
      "F-Measure: 70.667\n",
      "Accuracy score: 76.600\n",
      "\n",
      "Confustion matrix: \n",
      "[[ 0  0  0  0  0  0  0  1  0]\n",
      " [ 0 50  0  0 11  0  1  8  1]\n",
      " [ 0  1  3  2  1  0  1  9  0]\n",
      " [ 0  1  1 44  3  0  0  5  0]\n",
      " [ 0  3  0  1 99  0  3 20  0]\n",
      " [ 0  0  0  0  1  0  0  0  0]\n",
      " [ 0  1  0  0 11  0 77  6  0]\n",
      " [ 0  2  0  1 14  0  5 99  0]\n",
      " [ 0  0  0  0  1  0  1  1 11]]\n"
     ]
    }
   ],
   "source": [
    "X_doc2fast = np.array([get_embedding_sentence(question) for question in dataset['clean_questions'].values])\n",
    "le = LabelEncoder()\n",
    "y_doc2fast = le.fit_transform(dataset['a'])\n",
    "\n",
    "train_with_svm(X_doc2fast, get_encoded_y('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embedding Accuracy Fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold - 1 - SVM Accuracy Score ->  - 38.80\n",
      "Fold - 2 - SVM Accuracy Score ->  - 41.40\n",
      "Fold - 3 - SVM Accuracy Score ->  - 41.40\n",
      "Fold - 4 - SVM Accuracy Score ->  - 36.80\n",
      "Fold - 5 - SVM Accuracy Score ->  - 35.80\n",
      "Fold - 6 - SVM Accuracy Score ->  - 38.80\n",
      "Fold - 7 - SVM Accuracy Score ->  - 34.80\n",
      "Fold - 8 - SVM Accuracy Score ->  - 36.00\n",
      "Fold - 9 - SVM Accuracy Score ->  - 37.60\n",
      "Fold - 10 - SVM Accuracy Score ->  - 38.20\n",
      "Mean Accuracy 37.96 \n",
      "Std Accuracy 2.12\n",
      "\n",
      "\n",
      "Best accuracy : 41.4\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 41.400\n",
      "\n",
      "Confustion matrix: \n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "train_with_svm(X_doc2fast, get_encoded_y('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - A NN classifier s.a. an LSTM for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The maximum number of words to be used. (most frequent)\n",
    "MAX_NB_WORDS = 5000\n",
    "# Max number of words in each complaint.\n",
    "MAX_SEQUENCE_LENGTH = 25\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 160\n",
    "epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5582 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, split=' ')\n",
    "tokenizer.fit_on_texts(dataset['questions'].values)\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 7, 2, 321, 105, 31, 1837, 17, 68, 9, 20, 71, 6, 194, 48, 32, 22, 376, 111]\n",
      "Shape of data tensor: (5000, 25)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X = tokenizer.texts_to_sequences(dataset['questions'].values)\n",
    "print(X[0])\n",
    "X_lstm = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "print('Shape of data tensor:', X_lstm.shape)\n",
    "\n",
    "\n",
    "def get_dummi_y(_type='a'):\n",
    "    y = pd.get_dummies(dataset[_type]).values\n",
    "    print('Shape of label tensor:', y.shape)\n",
    "    return y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (5000, 79)\n",
      "WARNING:tensorflow:Layer lstm_33 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_33 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_33 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_33 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f794c43cfd0>"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Embedding, SpatialDropout1D, LSTM, Dense\n",
    "from tensorflow.keras import Sequential\n",
    "\n",
    "def get_lstm_model(X, y, verbose=0):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "    model.add(SpatialDropout1D(0.2))\n",
    "    model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
    "    # model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\n",
    "    model.add(Dense(y.shape[1], activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    if verbose == 1:\n",
    "        print(model.summary())\n",
    "    return model\n",
    "\n",
    "get_lstm_model(X_lstm, get_dummi_y('b'), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4500, 25) (4500, 10)\n",
      "(500, 25) (500, 10)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_lstm,y_nn, test_size = 0.10, random_state = 42)\n",
    "print(X_train.shape,Y_train.shape)\n",
    "print(X_test.shape,Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64\n",
    "\n",
    "def train_LSTM(X, y):\n",
    "    best_prediction = None\n",
    "    best_test = None\n",
    "    best_accuracy = 0\n",
    "\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    fold = 0\n",
    "    accuracies = []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        fold += 1\n",
    "        print(\"FOLD {}\".format(fold))\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model = get_lstm_model(X, y, 0)\n",
    "\n",
    "        hist = model.fit(X_train, y_train, verbose=0, epochs=epochs, batch_size=batch_size,validation_split=0.1,callbacks=[EarlyStopping(monitor='val_loss', patience=3, min_delta=0.0001)])\n",
    "        predictions = model.predict(X_test)\n",
    "        for item in hist.history.items():\n",
    "            mean_val = np.mean(item[1])\n",
    "            if item[0] == 'accuracy':\n",
    "                acc = mean_val\n",
    "            print(\"Mean {} : {}\".format(item[0], mean_val))\n",
    "        \n",
    "        \n",
    "        print(\"\\n\")\n",
    "        if best_accuracy < acc:\n",
    "            best_accuracy = acc\n",
    "            best_prediction = predictions\n",
    "            best_test = y_test\n",
    "            \n",
    "    fine_pred = [np.argmax(p) for p in best_prediction]\n",
    "    fine_gt = [np.argmax(p) for p in best_test]\n",
    "    PRC_matrics(fine_pred, fine_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Accuracy Coarse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (5000, 10)\n",
      "WARNING:tensorflow:Layer lstm_17 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 3s 54ms/step - loss: 1.8183 - accuracy: 0.2711 - val_loss: 1.9285 - val_accuracy: 0.1511\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 3s 52ms/step - loss: 1.2373 - accuracy: 0.5617 - val_loss: 1.2636 - val_accuracy: 0.5756\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 3s 51ms/step - loss: 0.6065 - accuracy: 0.8084 - val_loss: 0.8464 - val_accuracy: 0.7400\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 3s 52ms/step - loss: 0.3169 - accuracy: 0.9089 - val_loss: 0.7515 - val_accuracy: 0.7756\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 55ms/step - loss: 0.1771 - accuracy: 0.9511 - val_loss: 0.8035 - val_accuracy: 0.7533\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 3s 51ms/step - loss: 0.0982 - accuracy: 0.9768 - val_loss: 0.8636 - val_accuracy: 0.7489\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 3s 53ms/step - loss: 0.0680 - accuracy: 0.9825 - val_loss: 0.8659 - val_accuracy: 0.7644\n",
      "Mean loss : 0.6174741334148816\n",
      "Mean accuracy : 0.7800705560616085\n",
      "Mean val_loss : 1.0461519019944328\n",
      "Mean val_accuracy : 0.6441269921404975\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_18 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 57ms/step - loss: 1.7957 - accuracy: 0.2822 - val_loss: 1.7176 - val_accuracy: 0.3867\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 3s 54ms/step - loss: 1.2522 - accuracy: 0.5385 - val_loss: 1.4359 - val_accuracy: 0.4933\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 3s 50ms/step - loss: 0.6645 - accuracy: 0.7852 - val_loss: 1.0689 - val_accuracy: 0.6511\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 3s 52ms/step - loss: 0.3244 - accuracy: 0.9032 - val_loss: 0.6991 - val_accuracy: 0.8000\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 3s 54ms/step - loss: 0.1739 - accuracy: 0.9509 - val_loss: 0.7672 - val_accuracy: 0.7978\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 3s 53ms/step - loss: 0.0989 - accuracy: 0.9731 - val_loss: 0.9423 - val_accuracy: 0.7133\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 56ms/step - loss: 0.0669 - accuracy: 0.9830 - val_loss: 0.9849 - val_accuracy: 0.7156\n",
      "Mean loss : 0.6252083576151303\n",
      "Mean accuracy : 0.7737213373184204\n",
      "Mean val_loss : 1.0879763194492884\n",
      "Mean val_accuracy : 0.6511111089161464\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_19 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 1.7965 - accuracy: 0.2906 - val_loss: 1.9025 - val_accuracy: 0.1622\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 3s 53ms/step - loss: 1.1866 - accuracy: 0.5679 - val_loss: 1.6169 - val_accuracy: 0.3978\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.6108 - accuracy: 0.8086 - val_loss: 1.0094 - val_accuracy: 0.6889\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.3063 - accuracy: 0.9114 - val_loss: 0.7645 - val_accuracy: 0.7867\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.1659 - accuracy: 0.9553 - val_loss: 0.6557 - val_accuracy: 0.8178\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.1024 - accuracy: 0.9738 - val_loss: 0.8307 - val_accuracy: 0.7600\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.0647 - accuracy: 0.9835 - val_loss: 0.8119 - val_accuracy: 0.7733\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.0528 - accuracy: 0.9884 - val_loss: 0.9527 - val_accuracy: 0.7689\n",
      "Mean loss : 0.5357442414388061\n",
      "Mean accuracy : 0.8099382668733597\n",
      "Mean val_loss : 1.068048857152462\n",
      "Mean val_accuracy : 0.6444444414228201\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_20 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 1.8048 - accuracy: 0.2864 - val_loss: 1.7644 - val_accuracy: 0.2267\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 1.2442 - accuracy: 0.5469 - val_loss: 1.4851 - val_accuracy: 0.4378\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.6258 - accuracy: 0.8027 - val_loss: 0.9965 - val_accuracy: 0.6711\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.3248 - accuracy: 0.9081 - val_loss: 0.8186 - val_accuracy: 0.7644\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.1788 - accuracy: 0.9528 - val_loss: 0.6823 - val_accuracy: 0.7889\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.1027 - accuracy: 0.9736 - val_loss: 0.7536 - val_accuracy: 0.7800\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.0665 - accuracy: 0.9832 - val_loss: 0.8277 - val_accuracy: 0.7733\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.0488 - accuracy: 0.9877 - val_loss: 1.0879 - val_accuracy: 0.6756\n",
      "Mean loss : 0.5495527326129377\n",
      "Mean accuracy : 0.8051851838827133\n",
      "Mean val_loss : 1.0520197451114655\n",
      "Mean val_accuracy : 0.6397222150117159\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_21 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 1.8187 - accuracy: 0.2780 - val_loss: 1.9432 - val_accuracy: 0.1667\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 1.1977 - accuracy: 0.5788 - val_loss: 1.2013 - val_accuracy: 0.6111\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.6027 - accuracy: 0.8099 - val_loss: 0.8262 - val_accuracy: 0.7644\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.3226 - accuracy: 0.9067 - val_loss: 0.7639 - val_accuracy: 0.7667\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.1749 - accuracy: 0.9509 - val_loss: 0.8364 - val_accuracy: 0.7356\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.1114 - accuracy: 0.9723 - val_loss: 0.7864 - val_accuracy: 0.7622\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.0596 - accuracy: 0.9857 - val_loss: 0.9490 - val_accuracy: 0.7067\n",
      "Mean loss : 0.6125149205327034\n",
      "Mean accuracy : 0.7831746126924243\n",
      "Mean val_loss : 1.043763816356659\n",
      "Mean val_accuracy : 0.644761900816645\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_22 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 1.7959 - accuracy: 0.2830 - val_loss: 1.9444 - val_accuracy: 0.1622\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 1.2121 - accuracy: 0.5741 - val_loss: 1.4476 - val_accuracy: 0.4911\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.6013 - accuracy: 0.8114 - val_loss: 0.8093 - val_accuracy: 0.7511\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.2911 - accuracy: 0.9156 - val_loss: 0.8573 - val_accuracy: 0.7467\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.1694 - accuracy: 0.9536 - val_loss: 0.7723 - val_accuracy: 0.7644\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.1033 - accuracy: 0.9723 - val_loss: 0.8474 - val_accuracy: 0.7711\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.0632 - accuracy: 0.9847 - val_loss: 0.9712 - val_accuracy: 0.7400\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.0470 - accuracy: 0.9879 - val_loss: 0.9004 - val_accuracy: 0.7689\n",
      "Mean loss : 0.5354137560352683\n",
      "Mean accuracy : 0.8103086389601231\n",
      "Mean val_loss : 1.068724848330021\n",
      "Mean val_accuracy : 0.6494444478303194\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_23 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 1.7946 - accuracy: 0.2874 - val_loss: 1.8243 - val_accuracy: 0.1644\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 1.2465 - accuracy: 0.5358 - val_loss: 1.3047 - val_accuracy: 0.6156\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.6315 - accuracy: 0.7901 - val_loss: 1.1550 - val_accuracy: 0.5911\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.3347 - accuracy: 0.9020 - val_loss: 0.7875 - val_accuracy: 0.7711\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.1768 - accuracy: 0.9479 - val_loss: 0.8052 - val_accuracy: 0.7489\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.1072 - accuracy: 0.9686 - val_loss: 0.7287 - val_accuracy: 0.7733\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.0746 - accuracy: 0.9847 - val_loss: 0.7915 - val_accuracy: 0.7556\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.0652 - accuracy: 0.9825 - val_loss: 0.8679 - val_accuracy: 0.7444\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.0330 - accuracy: 0.9923 - val_loss: 0.9383 - val_accuracy: 0.7489\n",
      "Mean loss : 0.49600091204047203\n",
      "Mean accuracy : 0.8212620152367486\n",
      "Mean val_loss : 1.0225619938638475\n",
      "Mean val_accuracy : 0.6570370462205675\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_24 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 1.8099 - accuracy: 0.2795 - val_loss: 1.7321 - val_accuracy: 0.1267\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 1.2254 - accuracy: 0.5462 - val_loss: 1.4637 - val_accuracy: 0.4644\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.5733 - accuracy: 0.8160 - val_loss: 0.7112 - val_accuracy: 0.8044\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.2895 - accuracy: 0.9200 - val_loss: 0.8806 - val_accuracy: 0.7111\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 57ms/step - loss: 0.1624 - accuracy: 0.9558 - val_loss: 0.7187 - val_accuracy: 0.7756\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.0954 - accuracy: 0.9760 - val_loss: 0.6879 - val_accuracy: 0.7911\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.0690 - accuracy: 0.9842 - val_loss: 0.7881 - val_accuracy: 0.7778\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.0510 - accuracy: 0.9891 - val_loss: 0.8067 - val_accuracy: 0.7800\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.0301 - accuracy: 0.9928 - val_loss: 0.8225 - val_accuracy: 0.7622\n",
      "Mean loss : 0.4784498129867845\n",
      "Mean accuracy : 0.8288614584339989\n",
      "Mean val_loss : 0.9568546149465773\n",
      "Mean val_accuracy : 0.6659259266323514\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_25 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 6s 91ms/step - loss: 1.8202 - accuracy: 0.2862 - val_loss: 1.7360 - val_accuracy: 0.1489\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 1.2514 - accuracy: 0.5435 - val_loss: 1.6379 - val_accuracy: 0.3289\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.5839 - accuracy: 0.8081 - val_loss: 0.8765 - val_accuracy: 0.7511\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.2795 - accuracy: 0.9156 - val_loss: 0.8502 - val_accuracy: 0.7489\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.1588 - accuracy: 0.9528 - val_loss: 0.7059 - val_accuracy: 0.7933\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.0898 - accuracy: 0.9758 - val_loss: 0.9366 - val_accuracy: 0.7289\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.0612 - accuracy: 0.9864 - val_loss: 0.8459 - val_accuracy: 0.7578\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 58ms/step - loss: 0.0524 - accuracy: 0.9864 - val_loss: 1.0021 - val_accuracy: 0.7200\n",
      "Mean loss : 0.5371598335914314\n",
      "Mean accuracy : 0.8068518452346325\n",
      "Mean val_loss : 1.0739071443676949\n",
      "Mean val_accuracy : 0.6222222223877907\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_26 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 1.8166 - accuracy: 0.2709 - val_loss: 1.7837 - val_accuracy: 0.1511\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 1.3508 - accuracy: 0.4983 - val_loss: 1.4563 - val_accuracy: 0.4622\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.7419 - accuracy: 0.7620 - val_loss: 1.1927 - val_accuracy: 0.5889\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 0.3950 - accuracy: 0.8812 - val_loss: 1.0417 - val_accuracy: 0.6511\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.2257 - accuracy: 0.9351 - val_loss: 0.6796 - val_accuracy: 0.7978\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 64ms/step - loss: 0.1297 - accuracy: 0.9672 - val_loss: 0.7386 - val_accuracy: 0.8000\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.0836 - accuracy: 0.9783 - val_loss: 1.1481 - val_accuracy: 0.7356\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 62ms/step - loss: 0.0680 - accuracy: 0.9830 - val_loss: 1.1046 - val_accuracy: 0.7067\n",
      "Mean loss : 0.6014089845120907\n",
      "Mean accuracy : 0.784475315362215\n",
      "Mean val_loss : 1.1431562080979347\n",
      "Mean val_accuracy : 0.6116666626185179\n",
      "\n",
      "\n",
      "\n",
      "Precision: 77.215\n",
      "Recall: 81.333\n",
      "F-Measure: 79.221\n",
      "Accuracy score: 81.400\n",
      "\n",
      "Confustion matrix: \n",
      "[[ 50   0   0   5   0   2   2   0   0]\n",
      " [  0  11   0   0   0   0   5   0   0]\n",
      " [  0   3  56   2   0   0   4   0   0]\n",
      " [  6   0   2  92   0   4  14   0   0]\n",
      " [  0   0   0   0   0   0   0   0   0]\n",
      " [  2   0   0   5   0 101   4   0   1]\n",
      " [  4   2   3  12   1   3  85   0   4]\n",
      " [  1   0   0   0   0   0   0   0   0]\n",
      " [  0   0   0   1   0   0   1   0  12]]\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(X_lstm, get_dummi_y('a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Accuracy Fine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (5000, 79)\n",
      "WARNING:tensorflow:Layer lstm_34 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_34\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_34 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_34 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_34 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 3.8612 - accuracy: 0.0719 - val_loss: 3.5058 - val_accuracy: 0.3089\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 3.7169 - accuracy: 0.0746 - val_loss: 3.6009 - val_accuracy: 0.0133\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 3.4510 - accuracy: 0.1358 - val_loss: 3.3334 - val_accuracy: 0.1378\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 2.8232 - accuracy: 0.2965 - val_loss: 2.7720 - val_accuracy: 0.2711\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 59ms/step - loss: 2.0225 - accuracy: 0.4958 - val_loss: 2.2109 - val_accuracy: 0.4444\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 1.4015 - accuracy: 0.6474 - val_loss: 2.0559 - val_accuracy: 0.5133\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 60ms/step - loss: 0.9880 - accuracy: 0.7541 - val_loss: 2.1231 - val_accuracy: 0.4733\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 61ms/step - loss: 0.6985 - accuracy: 0.8338 - val_loss: 1.9958 - val_accuracy: 0.5733\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 66ms/step - loss: 0.5085 - accuracy: 0.8815 - val_loss: 2.0679 - val_accuracy: 0.5200\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 4s 64ms/step - loss: 0.3997 - accuracy: 0.9094 - val_loss: 2.1377 - val_accuracy: 0.5156\n",
      "Mean loss : 1.9871169924736023\n",
      "Mean accuracy : 0.5100740700960159\n",
      "Mean val_loss : 2.580352485179901\n",
      "Mean val_accuracy : 0.3771111054345965\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_35 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_35\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_35 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_35 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_35 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_33 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 5s 74ms/step - loss: 3.8535 - accuracy: 0.0716 - val_loss: 3.4839 - val_accuracy: 0.3133\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 3.7212 - accuracy: 0.0842 - val_loss: 3.5813 - val_accuracy: 0.0133\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 3.4180 - accuracy: 0.1669 - val_loss: 3.0484 - val_accuracy: 0.3244\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 2.4781 - accuracy: 0.3775 - val_loss: 2.9302 - val_accuracy: 0.2444\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 1.7612 - accuracy: 0.5486 - val_loss: 2.3308 - val_accuracy: 0.4378\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 1.2433 - accuracy: 0.6872 - val_loss: 2.0911 - val_accuracy: 0.4978\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.8972 - accuracy: 0.7768 - val_loss: 2.2408 - val_accuracy: 0.4156\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.6705 - accuracy: 0.8390 - val_loss: 2.2673 - val_accuracy: 0.4667\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 0.5202 - accuracy: 0.8775 - val_loss: 2.2077 - val_accuracy: 0.4911\n",
      "Mean loss : 2.0625699758529663\n",
      "Mean accuracy : 0.492153637111187\n",
      "Mean val_loss : 2.6868425475226507\n",
      "Mean val_accuracy : 0.35604938512874973\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_36 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_36\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_36 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_36 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_36 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_34 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 3.8646 - accuracy: 0.0768 - val_loss: 3.5625 - val_accuracy: 0.3089\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 5s 72ms/step - loss: 3.7172 - accuracy: 0.0909 - val_loss: 3.5223 - val_accuracy: 0.2111\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 3.4202 - accuracy: 0.1565 - val_loss: 3.3292 - val_accuracy: 0.2133\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 2.6345 - accuracy: 0.3407 - val_loss: 2.5925 - val_accuracy: 0.3622\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 1.9166 - accuracy: 0.5299 - val_loss: 2.9176 - val_accuracy: 0.3556\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 1.3498 - accuracy: 0.6610 - val_loss: 2.3036 - val_accuracy: 0.4556\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 0.9674 - accuracy: 0.7632 - val_loss: 2.1609 - val_accuracy: 0.4822\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 0.7298 - accuracy: 0.8205 - val_loss: 2.1550 - val_accuracy: 0.5222\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 0.5474 - accuracy: 0.8694 - val_loss: 2.4961 - val_accuracy: 0.4044\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 0.4062 - accuracy: 0.9049 - val_loss: 2.3268 - val_accuracy: 0.4956\n",
      "Mean loss : 1.9553802877664566\n",
      "Mean accuracy : 0.5213827192783356\n",
      "Mean val_loss : 2.7366435050964357\n",
      "Mean val_accuracy : 0.3811111137270927\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_37 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_37\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_37 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_37 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_37 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 75ms/step - loss: 3.8567 - accuracy: 0.0741 - val_loss: 3.5396 - val_accuracy: 0.3133\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 3.7073 - accuracy: 0.0869 - val_loss: 3.5304 - val_accuracy: 0.3000\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 3.3542 - accuracy: 0.1788 - val_loss: 3.1469 - val_accuracy: 0.2311\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 2.5040 - accuracy: 0.3788 - val_loss: 2.4144 - val_accuracy: 0.3689\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 5s 72ms/step - loss: 1.6865 - accuracy: 0.5857 - val_loss: 2.0842 - val_accuracy: 0.4844\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 1.1765 - accuracy: 0.7042 - val_loss: 2.1533 - val_accuracy: 0.4711\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 5s 70ms/step - loss: 0.8629 - accuracy: 0.7874 - val_loss: 2.0697 - val_accuracy: 0.5178\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 0.6266 - accuracy: 0.8477 - val_loss: 2.1145 - val_accuracy: 0.5267\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.4747 - accuracy: 0.8928 - val_loss: 2.4524 - val_accuracy: 0.4178\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.3793 - accuracy: 0.9123 - val_loss: 2.1672 - val_accuracy: 0.5222\n",
      "Mean loss : 1.8628714382648468\n",
      "Mean accuracy : 0.5448641978204251\n",
      "Mean val_loss : 2.567262887954712\n",
      "Mean val_accuracy : 0.41533333361148833\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_38 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_38 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_38 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_38 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 5s 74ms/step - loss: 3.8667 - accuracy: 0.0743 - val_loss: 3.5859 - val_accuracy: 0.2956\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 3.6778 - accuracy: 0.0921 - val_loss: 3.3479 - val_accuracy: 0.3000\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 3.2873 - accuracy: 0.1800 - val_loss: 3.0510 - val_accuracy: 0.2622\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 2.5647 - accuracy: 0.3543 - val_loss: 2.4103 - val_accuracy: 0.3822\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 1.8357 - accuracy: 0.5281 - val_loss: 2.3097 - val_accuracy: 0.4444\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 1.2948 - accuracy: 0.6714 - val_loss: 2.1315 - val_accuracy: 0.4756\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 0.9074 - accuracy: 0.7684 - val_loss: 2.0651 - val_accuracy: 0.4667\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 0.6650 - accuracy: 0.8442 - val_loss: 2.1075 - val_accuracy: 0.5067\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 0.4821 - accuracy: 0.8864 - val_loss: 1.9965 - val_accuracy: 0.5044\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.3661 - accuracy: 0.9158 - val_loss: 2.1499 - val_accuracy: 0.5044\n",
      "Mean loss : 1.8947577625513077\n",
      "Mean accuracy : 0.5315061785280705\n",
      "Mean val_loss : 2.5155413269996645\n",
      "Mean val_accuracy : 0.4142222225666046\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_39 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_39 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_39 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_39 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_37 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 5s 73ms/step - loss: 3.8418 - accuracy: 0.0758 - val_loss: 3.5477 - val_accuracy: 0.0111\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 3.6845 - accuracy: 0.0837 - val_loss: 3.4993 - val_accuracy: 0.0356\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 3.3459 - accuracy: 0.1657 - val_loss: 3.0996 - val_accuracy: 0.3067\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 2.6321 - accuracy: 0.3323 - val_loss: 2.5815 - val_accuracy: 0.3644\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 1.8790 - accuracy: 0.5178 - val_loss: 2.3370 - val_accuracy: 0.4156\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 1.3613 - accuracy: 0.6402 - val_loss: 2.1582 - val_accuracy: 0.4511\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 5s 72ms/step - loss: 0.9549 - accuracy: 0.7669 - val_loss: 2.0429 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.6974 - accuracy: 0.8289 - val_loss: 2.0941 - val_accuracy: 0.4956\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.5249 - accuracy: 0.8672 - val_loss: 2.1464 - val_accuracy: 0.5044\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.4068 - accuracy: 0.9057 - val_loss: 2.2771 - val_accuracy: 0.4644\n",
      "Mean loss : 1.9328479886054992\n",
      "Mean accuracy : 0.5184197545051574\n",
      "Mean val_loss : 2.578385353088379\n",
      "Mean val_accuracy : 0.354888887796551\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_40 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_40\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_40 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_40 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_40 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 5s 73ms/step - loss: 3.8589 - accuracy: 0.0649 - val_loss: 3.5074 - val_accuracy: 0.3222\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 3.7112 - accuracy: 0.0899 - val_loss: 3.4068 - val_accuracy: 0.3222\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 5s 73ms/step - loss: 3.4537 - accuracy: 0.1464 - val_loss: 3.4992 - val_accuracy: 0.1244\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 2.6174 - accuracy: 0.3457 - val_loss: 2.7825 - val_accuracy: 0.3556\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 1.7242 - accuracy: 0.5691 - val_loss: 2.3274 - val_accuracy: 0.4400\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 1.1458 - accuracy: 0.7000 - val_loss: 2.1287 - val_accuracy: 0.5089\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 0.7882 - accuracy: 0.8012 - val_loss: 2.2878 - val_accuracy: 0.4733\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 0.5802 - accuracy: 0.8553 - val_loss: 2.1253 - val_accuracy: 0.4689\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.4393 - accuracy: 0.8993 - val_loss: 2.2531 - val_accuracy: 0.4822\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.3194 - accuracy: 0.9277 - val_loss: 2.2838 - val_accuracy: 0.5222\n",
      "Mean loss : 1.8638234287500381\n",
      "Mean accuracy : 0.5399506136775016\n",
      "Mean val_loss : 2.660208749771118\n",
      "Mean val_accuracy : 0.4020000033080578\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_41 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_41\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_41 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_41 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_41 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 5s 72ms/step - loss: 3.8592 - accuracy: 0.0721 - val_loss: 3.5554 - val_accuracy: 0.0111\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 3.7178 - accuracy: 0.0879 - val_loss: 3.5304 - val_accuracy: 0.1711\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 3.4143 - accuracy: 0.1753 - val_loss: 3.1307 - val_accuracy: 0.2200\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 2.6846 - accuracy: 0.3249 - val_loss: 2.6106 - val_accuracy: 0.3444\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 1.9144 - accuracy: 0.5232 - val_loss: 1.9808 - val_accuracy: 0.5089\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 1.3350 - accuracy: 0.6620 - val_loss: 1.9541 - val_accuracy: 0.5289\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 66ms/step - loss: 0.9942 - accuracy: 0.7481 - val_loss: 2.1329 - val_accuracy: 0.4733\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.7119 - accuracy: 0.8269 - val_loss: 2.0972 - val_accuracy: 0.4711\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 5s 72ms/step - loss: 0.5220 - accuracy: 0.8788 - val_loss: 2.0910 - val_accuracy: 0.5178\n",
      "Mean loss : 2.1281506485409207\n",
      "Mean accuracy : 0.4776954717106289\n",
      "Mean val_loss : 2.5648055738872952\n",
      "Mean val_accuracy : 0.3607407445088029\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_42 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_42\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_42 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_42 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_42 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_40 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 5s 76ms/step - loss: 3.8656 - accuracy: 0.0753 - val_loss: 3.5412 - val_accuracy: 0.0422\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 3.7210 - accuracy: 0.0928 - val_loss: 3.6491 - val_accuracy: 0.0111\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 3.4293 - accuracy: 0.1477 - val_loss: 3.4261 - val_accuracy: 0.1400\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 2.7395 - accuracy: 0.3012 - val_loss: 3.2094 - val_accuracy: 0.2533\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 65ms/step - loss: 1.9550 - accuracy: 0.5091 - val_loss: 2.2688 - val_accuracy: 0.4800\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 1.3498 - accuracy: 0.6654 - val_loss: 2.1700 - val_accuracy: 0.4689\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.9413 - accuracy: 0.7632 - val_loss: 2.1861 - val_accuracy: 0.5111\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.6809 - accuracy: 0.8378 - val_loss: 2.2017 - val_accuracy: 0.4400\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 0.5029 - accuracy: 0.8872 - val_loss: 2.2941 - val_accuracy: 0.4933\n",
      "Mean loss : 2.1316713160938687\n",
      "Mean accuracy : 0.4755281201667256\n",
      "Mean val_loss : 2.771830426322089\n",
      "Mean val_accuracy : 0.3155555564703213\n",
      "\n",
      "\n",
      "\n",
      "WARNING:tensorflow:Layer lstm_43 will not use cuDNN kernel since it doesn't meet the cuDNN kernel criteria. It will use generic GPU kernel as fallback when running on GPU\n",
      "Model: \"sequential_43\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_43 (Embedding)     (None, 25, 160)           800000    \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_43 (Spatia (None, 25, 160)           0         \n",
      "_________________________________________________________________\n",
      "lstm_43 (LSTM)               (None, 196)               279888    \n",
      "_________________________________________________________________\n",
      "dense_41 (Dense)             (None, 79)                15563     \n",
      "=================================================================\n",
      "Total params: 1,095,451\n",
      "Trainable params: 1,095,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 5s 74ms/step - loss: 3.8692 - accuracy: 0.0691 - val_loss: 3.4925 - val_accuracy: 0.3067\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 66ms/step - loss: 3.7096 - accuracy: 0.0869 - val_loss: 3.4826 - val_accuracy: 0.2778\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 3.3755 - accuracy: 0.1780 - val_loss: 3.0556 - val_accuracy: 0.2956\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 2.5791 - accuracy: 0.3538 - val_loss: 2.6464 - val_accuracy: 0.3356\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 69ms/step - loss: 1.7775 - accuracy: 0.5551 - val_loss: 2.2723 - val_accuracy: 0.4511\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 67ms/step - loss: 1.2642 - accuracy: 0.6825 - val_loss: 2.1533 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 4s 68ms/step - loss: 0.9125 - accuracy: 0.7723 - val_loss: 2.3308 - val_accuracy: 0.4422\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 5s 72ms/step - loss: 0.6843 - accuracy: 0.8351 - val_loss: 2.1545 - val_accuracy: 0.4578\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.5213 - accuracy: 0.8719 - val_loss: 2.2737 - val_accuracy: 0.4489\n",
      "Mean loss : 2.0770145853360495\n",
      "Mean accuracy : 0.4894101512100961\n",
      "Mean val_loss : 2.6513061788347034\n",
      "Mean val_accuracy : 0.39061728450987077\n",
      "\n",
      "\n",
      "\n",
      "Precision: 0.000\n",
      "Recall: 0.000\n",
      "F-Measure: nan\n",
      "Accuracy score: 57.000\n",
      "\n",
      "Confustion matrix: \n",
      "[[ 2  0  0 ...  0  0  0]\n",
      " [ 0  4  1 ...  0  0  0]\n",
      " [ 0  0 23 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  1  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dulanj/Environments/py376/lib/python3.7/site-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in double_scalars\n"
     ]
    }
   ],
   "source": [
    "train_LSTM(X_lstm, get_dummi_y('b'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4. BONUS - experiment with a BERT-based classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "import bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1, Total size: 423.26MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a bert tockenizer\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "os.environ['TFHUB_DOWNLOAD_PROGRESS'] = \"1\"\n",
    "os.environ[\"TFHUB_CACHE_DIR\"] = \"/tmp/model\"\n",
    "\n",
    "BertTokenizer = bert.bert_tokenization.FullTokenizer\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",\n",
    "                            trainable=False)\n",
    "vocabulary_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "to_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = BertTokenizer(vocabulary_file, to_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_reviews(text_reviews):\n",
    "    return tokenizer.convert_tokens_to_ids(tokenizer.tokenize(text_reviews))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_questions = [tokenize_reviews(qu) for qu in dataset['questions'].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "bret_y_corse = get_encoded_y('a')\n",
    "bret_y_fine = get_encoded_y('b')\n",
    "\n",
    "bert_train_dataset_c = [[question, bret_y_corse[i], len(question)] for i, question in enumerate(tokenized_questions)]\n",
    "bert_train_dataset_f = [[question, bret_y_fine[i], len(question)] for i, question in enumerate(tokenized_questions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(bert_train_dataset)\n",
    "sorted_bert_train_dataset_c = [(bert_train[0], bert_train[1]) for bert_train in bert_train_dataset_c]\n",
    "sorted_bert_train_dataset_f = [(bert_train[0], bert_train[1]) for bert_train in bert_train_dataset_f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "processed_dataset_c = tf.data.Dataset.from_generator(lambda: sorted_bert_train_dataset_c, output_types=(tf.int32, tf.int32))\n",
    "processed_dataset_f = tf.data.Dataset.from_generator(lambda: sorted_bert_train_dataset_f, output_types=(tf.int32, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "batched_dataset_c = processed_dataset_c.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))\n",
    "batched_dataset_f = processed_dataset_f.padded_batch(BATCH_SIZE, padded_shapes=((None, ), ()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TEXT_MODEL(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 vocabulary_size,\n",
    "                 embedding_dimensions=128,\n",
    "                 cnn_filters=50,\n",
    "                 dnn_units=512,\n",
    "                 model_output_classes=2,\n",
    "                 dropout_rate=0.1,\n",
    "                 training=False,\n",
    "                 name=\"text_model\"):\n",
    "        super(TEXT_MODEL, self).__init__(name=name)\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocabulary_size,\n",
    "                                          embedding_dimensions)\n",
    "        self.cnn_layer1 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=2,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer2 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=3,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.cnn_layer3 = layers.Conv1D(filters=cnn_filters,\n",
    "                                        kernel_size=4,\n",
    "                                        padding=\"valid\",\n",
    "                                        activation=\"relu\")\n",
    "        self.pool = layers.GlobalMaxPool1D()\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=dnn_units, activation=\"relu\")\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        if model_output_classes == 2:\n",
    "            self.last_dense = layers.Dense(units=1,\n",
    "                                           activation=\"sigmoid\")\n",
    "        else:\n",
    "            self.last_dense = layers.Dense(units=model_output_classes,\n",
    "                                           activation=\"softmax\")\n",
    "    \n",
    "    def call(self, inputs, training):\n",
    "        l = self.embedding(inputs)\n",
    "        l_1 = self.cnn_layer1(l) \n",
    "        l_1 = self.pool(l_1) \n",
    "        l_2 = self.cnn_layer2(l) \n",
    "        l_2 = self.pool(l_2)\n",
    "        l_3 = self.cnn_layer3(l)\n",
    "        l_3 = self.pool(l_3) \n",
    "        \n",
    "        concatenated = tf.concat([l_1, l_2, l_3], axis=-1) # (batch_size, 3 * cnn_filters)\n",
    "        concatenated = self.dense_1(concatenated)\n",
    "        concatenated = self.dropout(concatenated, training)\n",
    "        model_output = self.last_dense(concatenated)\n",
    "        \n",
    "        return model_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_LENGTH = len(tokenizer.vocab)\n",
    "EMB_DIM = 200\n",
    "CNN_FILTERS = 100\n",
    "DNN_UNITS = 256\n",
    "OUTPUT_CLASSES = 10\n",
    "DROPOUT_RATE = 0.2\n",
    "NB_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_and_compile_bert(output_classes):\n",
    "    text_model = TEXT_MODEL(vocabulary_size=VOCAB_LENGTH,\n",
    "                            embedding_dimensions=EMB_DIM,\n",
    "                            cnn_filters=CNN_FILTERS,\n",
    "                            dnn_units=DNN_UNITS,\n",
    "                            model_output_classes=output_classes,\n",
    "                            dropout_rate=DROPOUT_RATE)\n",
    "    text_model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "                       optimizer=\"adam\",\n",
    "                       metrics=[\"sparse_categorical_accuracy\"])\n",
    "    \n",
    "    return text_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import math\n",
    "best_test = None\n",
    "def train_BERT(batched_dataset, output_classes):\n",
    "    \n",
    "    test_accuracies = []\n",
    "    best_prediction = None\n",
    "    best_test = None\n",
    "    best_accuracy = 0\n",
    "    acc =0\n",
    "    best_model = None\n",
    "    \n",
    "    TOTAL_BATCHES = math.ceil(len(sorted_bert_train_dataset_c) / BATCH_SIZE)\n",
    "    TEST_BATCHES = TOTAL_BATCHES // 10\n",
    "    batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "\n",
    "    cv = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    fold = 0\n",
    "    accuracies = []\n",
    "    for i in range(10):\n",
    "        fold += 1\n",
    "        print(\"FOLD {}\".format(fold))\n",
    "        text_model = create_and_compile_bert(output_classes)\n",
    "\n",
    "        # shuffel and take 10 batches\n",
    "        batched_dataset.shuffle(TOTAL_BATCHES)\n",
    "        test_data = batched_dataset.take(TEST_BATCHES)\n",
    "        train_data = batched_dataset.skip(TEST_BATCHES)\n",
    "\n",
    "\n",
    "        hist = text_model.fit(train_data, epochs=NB_EPOCHS, verbose=0)\n",
    "#         results = text_model.evaluate(test_data)\n",
    "#         print(results)\n",
    "        for item in hist.history.items():\n",
    "            mean_val = np.mean(item[1])\n",
    "            if item[0] == 'sparse_categorical_accuracy':\n",
    "                acc = mean_val\n",
    "            print(\"Mean {} : {}\".format(item[0], mean_val))\n",
    "        results = text_model.evaluate(test_data)\n",
    "        acc = results[1]*100\n",
    "        print(\"Test Accuracy: {}\\n\\n\".format(acc)\n",
    "        test_accuracies.append(acc)\n",
    "        if best_accuracy < acc:\n",
    "            best_accuracy = acc\n",
    "            best_prediction = text_model.predict(test_data)\n",
    "            best_test = test_data\n",
    "            best_model = text_model\n",
    "    \n",
    "    print(\"Overall test accuracy: {}\".format(np.mean(test_accuracies)))\n",
    "    return best_model\n",
    "#     print(best_test)\n",
    "#     bert_pred = [np.argmax(p) for p in best_prediction]\n",
    "#     bert_gt = [np.argmax(p) for p in best_test]\n",
    "#     PRC_matrics(bert_pred, bert_gt)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Mean loss : 0.2827250716043636\n",
      "Mean sparse_categorical_accuracy : 0.905951327085495\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.0577 - sparse_categorical_accuracy: 0.7875\n",
      "Test Accuracy: 78.75000238418579\n",
      "\n",
      "\n",
      "FOLD 2\n",
      "Mean loss : 0.2709576961584389\n",
      "Mean sparse_categorical_accuracy : 0.9097123891115189\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0134 - sparse_categorical_accuracy: 0.7875\n",
      "Test Accuracy: 78.75000238418579\n",
      "\n",
      "\n",
      "FOLD 3\n",
      "Mean loss : 0.27320860591717067\n",
      "Mean sparse_categorical_accuracy : 0.9076327443122864\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0460 - sparse_categorical_accuracy: 0.7750\n",
      "Test Accuracy: 77.49999761581421\n",
      "\n",
      "\n",
      "FOLD 4\n",
      "Mean loss : 0.2798503952100873\n",
      "Mean sparse_categorical_accuracy : 0.9070132791996002\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.0391 - sparse_categorical_accuracy: 0.7750\n",
      "Test Accuracy: 77.49999761581421\n",
      "\n",
      "\n",
      "FOLD 5\n",
      "Mean loss : 0.28253342108801005\n",
      "Mean sparse_categorical_accuracy : 0.9055088490247727\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0808 - sparse_categorical_accuracy: 0.7833\n",
      "Test Accuracy: 78.33333611488342\n",
      "\n",
      "\n",
      "FOLD 6\n",
      "Mean loss : 0.2730441984720528\n",
      "Mean sparse_categorical_accuracy : 0.908672571182251\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.0271 - sparse_categorical_accuracy: 0.7833\n",
      "Test Accuracy: 78.33333611488342\n",
      "\n",
      "\n",
      "FOLD 7\n",
      "Mean loss : 0.28220053799450395\n",
      "Mean sparse_categorical_accuracy : 0.9063938051462174\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0772 - sparse_categorical_accuracy: 0.7854\n",
      "Test Accuracy: 78.54166626930237\n",
      "\n",
      "\n",
      "FOLD 8\n",
      "Mean loss : 0.27858771923929454\n",
      "Mean sparse_categorical_accuracy : 0.9060840725898742\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 1.0288 - sparse_categorical_accuracy: 0.7729\n",
      "Test Accuracy: 77.29166746139526\n",
      "\n",
      "\n",
      "FOLD 9\n",
      "Mean loss : 0.2831989986822009\n",
      "Mean sparse_categorical_accuracy : 0.9053097397089005\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.0910 - sparse_categorical_accuracy: 0.7583\n",
      "Test Accuracy: 75.83333253860474\n",
      "\n",
      "\n",
      "FOLD 10\n",
      "Mean loss : 0.2875193042680621\n",
      "Mean sparse_categorical_accuracy : 0.9022787600755692\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 1.0969 - sparse_categorical_accuracy: 0.7729\n",
      "Test Accuracy: 77.29166746139526\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_model = train_BERT(batched_dataset_c, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FOLD 1\n",
      "Mean loss : 1.053017887659371\n",
      "Mean sparse_categorical_accuracy : 0.7568362832069397\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.5951 - sparse_categorical_accuracy: 0.5938\n",
      "Test Accuracy: 59.375\n",
      "\n",
      "\n",
      "FOLD 2\n",
      "Mean loss : 1.06547115072608\n",
      "Mean sparse_categorical_accuracy : 0.751703542470932\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2.5794 - sparse_categorical_accuracy: 0.5771\n",
      "Test Accuracy: 57.70833492279053\n",
      "\n",
      "\n",
      "FOLD 3\n",
      "Mean loss : 1.0485209930688142\n",
      "Mean sparse_categorical_accuracy : 0.7557522162795067\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.5863 - sparse_categorical_accuracy: 0.5750\n",
      "Test Accuracy: 57.499998807907104\n",
      "\n",
      "\n",
      "FOLD 4\n",
      "Mean loss : 1.0588902793824673\n",
      "Mean sparse_categorical_accuracy : 0.7546902634203434\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.6405 - sparse_categorical_accuracy: 0.5854\n",
      "Test Accuracy: 58.541667461395264\n",
      "\n",
      "\n",
      "FOLD 5\n",
      "Mean loss : 1.05250611230731\n",
      "Mean sparse_categorical_accuracy : 0.7565044283866882\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.5792 - sparse_categorical_accuracy: 0.5813\n",
      "Test Accuracy: 58.125001192092896\n",
      "\n",
      "\n",
      "FOLD 6\n",
      "Mean loss : 1.0637003231793642\n",
      "Mean sparse_categorical_accuracy : 0.7517477795481682\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.5529 - sparse_categorical_accuracy: 0.6229\n",
      "Test Accuracy: 62.29166388511658\n",
      "\n",
      "\n",
      "FOLD 7\n",
      "Mean loss : 1.044200337678194\n",
      "Mean sparse_categorical_accuracy : 0.7570575274527073\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2.4991 - sparse_categorical_accuracy: 0.5979\n",
      "Test Accuracy: 59.79166626930237\n",
      "\n",
      "\n",
      "FOLD 8\n",
      "Mean loss : 1.0906424306333065\n",
      "Mean sparse_categorical_accuracy : 0.748407082259655\n",
      "15/15 [==============================] - 0s 6ms/step - loss: 2.7382 - sparse_categorical_accuracy: 0.5854\n",
      "Test Accuracy: 58.541667461395264\n",
      "\n",
      "\n",
      "FOLD 9\n",
      "Mean loss : 1.0519455816596746\n",
      "Mean sparse_categorical_accuracy : 0.7561946868896484\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.5410 - sparse_categorical_accuracy: 0.5729\n",
      "Test Accuracy: 57.29166865348816\n",
      "\n",
      "\n",
      "FOLD 10\n",
      "Mean loss : 1.057963067293167\n",
      "Mean sparse_categorical_accuracy : 0.752367266267538\n",
      "15/15 [==============================] - 0s 5ms/step - loss: 2.4735 - sparse_categorical_accuracy: 0.6062\n",
      "Test Accuracy: 60.624998807907104\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.TEXT_MODEL at 0x7f76642aa890>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_BERT(batched_dataset_f, 79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction = text_model.predict(test_data)\n",
    "# print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 76.768\n",
      "Recall: 89.412\n",
      "F-Measure: 82.609\n",
      "Accuracy score: 81.400\n",
      "Confustion matrix: \n",
      "[[65  0  1  3  1  1  0]\n",
      " [ 0 11  1  1  0  1  0]\n",
      " [ 1  1 52  2  0  0  0]\n",
      " [ 2  1  1 84  4  9  1]\n",
      " [ 0  0  0 10 82  4  0]\n",
      " [ 8 10  5 12 11 99  2]\n",
      " [ 0  0  0  0  0  0 14]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(76.76767676767676, 89.41176470588236, 82.60869565217392, 81.39999999999999)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pred = [np.argmax(p) for p in predictions]\n",
    "bert_gt = [np.argmax(p) for p in y_test]\n",
    "PRC_matrics(bert_pred, bert_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
